
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Programatically understanding Adagrad &#8212; Prog-ML</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet">
  <link href="../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script kind="utterances">

    var commentsRunWhenDOMLoaded = cb => {
    if (document.readyState != 'loading') {
        cb()
    } else if (document.addEventListener) {
        document.addEventListener('DOMContentLoaded', cb)
    } else {
        document.attachEvent('onreadystatechange', function() {
        if (document.readyState == 'complete') cb()
        })
    }
}

var addUtterances = () => {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://utteranc.es/client.js";
    script.async = "async";

    script.setAttribute("repo", "prog-ml/prog-ml.github.io");
    script.setAttribute("issue-term", "pathname");
    script.setAttribute("theme", "github-light");
    script.setAttribute("label", "ðŸ’¬ comment");
    script.setAttribute("crossorigin", "anonymous");

    sections = document.querySelectorAll("div.section");
    if (sections !== null) {
        section = sections[sections.length-1];
        section.appendChild(script);
    }
}
commentsRunWhenDOMLoaded(addUtterances);
</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="&lt;no title&gt;" href="../others/2021-03-24-sigmoid.html" />
    <link rel="prev" title="Autograd in JAX and PyTorch" href="2022-02-09-autograd-pytorch-jax.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/prog-ml.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Prog-ML</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../intro.html">
   Prog-ML
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/mle_coin.html">
   MLE, MAP and Fully Bayesian (conjugate prior and MCMC) for coin toss
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/variational.html">
   Variational Inference from scratch in JAX
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Probability Basics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../probability/sample-space.html">
   Sample Space
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../probability/random-variable.html">
   Random Variables
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../probability/pmf.html">
   Probability Mass Function (PMF)
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Continuous Probability Distributions
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../probability/univariate-normal.html">
   Properties of RV
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../probability/univariate-normal-expectations.html">
   Derivations for moments of univariate normal distribution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../probability/mvn-introduction.html">
   Multivariate Normal Distribution: Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../probability/mvn-marginal.html">
   Multivariate Normal Distribution: Marginals
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Bayesian ML
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml/2021-03-23-bayesian-ml.html">
   Bayesian ML: Fundamentals
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml/2021-04-14-bayesian-linear-regression.html">
   Bayesian linear regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml/2021-03-29-bayesian-model-selection.html">
   Bayesian model selection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml/2021-03-27-Marginal-Likelihood-2.html">
   Marginal likelihood
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml/2021-03-31-derivation-of-marginal-likelihood.html">
   Marginal likelihood for Bayesian linear regression
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Sampling from Distributions
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../sampling/rejection-sampling-lr.html">
   Simple rejection sampling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../sampling/Metropolis-Hastings.html">
   Metropolis Hastings
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../sampling/2021-03-10-importance-sampling.html">
   Importance sampling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../sampling/2021-03-10-rejection-sampling.html">
   Rejection sampling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../sampling/2022-02-04-sampling-normal.html">
   Sampling from univariate and multivariate normal distributions using Box-Muller transform
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../sampling/2020-04-16-inverse-transform.html">
   Sampling from common distributions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../sampling/2014-05-01-gibbs-sampling.html">
   Gibbs sampling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../sampling/2014-07-01-mcmc_coins.html">
   Coin tosses and MCMC
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Graphical Models
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../graphical_models/2022-02-15-draw-graphical-models.html">
   Drawing graphical models
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Mixture Models
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../mixture_models/2022-02-14-GMM.html">
   GMM learning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Information Theory
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../information_theory/kl-divergence.html">
   Understanding KL-Divergence
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Variational Models
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../variational_models/2022-02-12-variational-inference.html">
   Variational Inference
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Bayesian ML with PyMC
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml_with_pymc/2021-03-11-blr-pymc.html">
   Bayesian linear regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml_with_pymc/2021-03-12-logistic-bayesian.html">
   Bayesian logistic regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../gaussian_processes/2021-03-16-GP-PyMC3.html">
   Gaussian process regression in PyMC
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../gaussian_processes/2021-03-17-lls-gp-pymc3.html">
   Local Lengthscale GP with PyMC
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Bayesian ML with Pyro
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml_with_pyro/2021-08-20-Bayesian.html">
   Probabilistic Programming in Pyro
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml_with_pyro/2022_02_17_pyro_linreg.html">
   Linear Regression using Pyro
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml_with_pyro/2022-02-20-condition-pyro.html">
   Pyro Conditioning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Bayesian ML with PyTorch
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml_with_pytorch/2022-02-09-pytorch-learn-normal.html">
   Maximum Likelihood Estimation (MLE) for parameters of univariate and multivariate normal distribution in PyTorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml_with_pytorch/2022-02-11-pytorch-learn-normal-map.html">
   Maximum A-Posteriori (MAP) for parameters of univariate and multivariate normal distribution in PyTorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml_with_pytorch/2022-02-17-ppca.html">
   Probabilstic PCA using PyTorch distributions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml_with_pytorch/2022-02-14-logistic-regression.html">
   Logistic Regression using PyTorch distributions
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Bayesian ML with Tensorflow Probability
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml_with_tfp/2022-01-26-tfp-distributions.html">
   Testing out some distributions in Tensorflow Probability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml_with_tfp/2022-02-07-coin-toss.html">
   Coin Toss (MLE, MAP, Fully Bayesian) in TF Probability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml_with_tfp/2022-01-28-tfp-linear-regression.html">
   Linear Regression in Tensorflow Probability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml_with_tfp/2022-02-05-lr.html">
   Linear Regression in TF Probability using JointDistributionCoroutineAutoBatched
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml_with_tfp/2022-02-05-simple-dgm.html">
   Simple Directed Graphical Models in TF Probability
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Bayesian ML with Julia
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml_with_julia/2021-09-01-Hello-Julia-Language.html">
   Linear Regression from scratch in Julia
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Gaussian Processes
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../gaussian_processes/2020-03-26-gp.html">
   Some experiments in Gaussian Processes Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../gaussian_processes/2021-04-15-LLS-GP.html">
   Local Lengthscale GP
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../gaussian_processes/2021-04-15-deep-gp-from-scratch.html">
   Deep Kernel Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../gaussian_processes/2021-04-16-ard-gp.html">
   Automatic relevance determination (ARD)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../gaussian_processes/2021-04-16-GP-vs-DeepGP.html">
   GP v/s Deep GP on 2d data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../gaussian_processes/2022-02-23-gp_rff.html">
   Gaussian Processes with Random Fourier Features
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../gaussian_processes/2020-03-29-param-learning.html">
   Learning Gaussian Process regression parameters using gradient descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../gaussian_processes/2021-09-03-param-learning-sgd.html">
   Learning Gaussian Process regression parameters using mini-batch stochastic gradient descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../gaussian_processes/2020-06-26-gp-understand.html">
   Understanding Kernels in Gaussian Processes Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../gaussian_processes/pyro-deep-gp.html">
   Deep Kernel Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../gaussian_processes/pyro-binary-classification.html">
   GP Classification
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Linear Regression
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../linear_regression/2022_02_21_coordinate_descent_failure.html">
   Coordinate descent failure example
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Generative Adversarial Networks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../gans/2021-05-31-GAN.html">
   A programming introduction to GANs
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Hidden Markov Models
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../hmms/2013-06-01-hmm_simulate.html">
   HMM Simulation for Unfair Casino Problem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../hmms/2013-07-01-hmm_continuous.html">
   HMM Simulation for Continuous HMM
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tensor Factorization
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../tensor_factorization/2017-04-19-nmf-out-matrix.html">
   Out of matrix non-negative matrix factorisation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tensor_factorization/2017-04-20-parafac-out-tensor.html">
   Out of Tensor factorisation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tensor_factorization/2017-08-13-mf-autograd-adagrad.html">
   Adagrad based matrix factorization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tensor_factorization/2017-04-21-constrained-nmf-cvx.html">
   Constrained Non-negative matrix factorisation using CVXPY
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Neural Networks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../neural_networks/2020-03-08-keras-neural-non-linear.html">
   Some Neural Network Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neural_networks/2020-02-28-xor-relu-vector.html">
   Learning neural network for XOR
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neural_networks/2020-03-02-linear-scratch.html">
   Neural Networks from scratch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neural_networks/2018-01-13-denoising.html">
   Signal denoising using RNNs in PyTorch
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Expectation Maximization
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../em/2014-06-01-em.html">
   Programatically understanding Expectation Maximization
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Recommender Systems
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../recommender_systems/2017-12-18-recommend-keras.html">
   Recommender Systems in Keras
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../recommender_systems/2017-12-29-neural-collaborative-filtering.html">
   Neural Networks for Collaborative Filtering
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Active Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../active_learning/2018-06-20-active-committee.html">
   Active Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../active_learning/2022_01_24_Query_by_Committee.html">
   Query by Committee
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../active_learning/2020-04-21-active-learning-with-bayesian-linear-regression.html">
   Active Learning with Bayesian Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../active_learning/2022-03-06-maximal-expected-error-reduction.html">
   Problem with Expected Model Change
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  JAX
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../jax/introduction-jax.html">
   Using PRNG key
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2022-02-09-autograd-pytorch-jax.html">
   Autograd in JAX and PyTorch
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Programatically understanding Adagrad
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Appendix 1 - Linear Algebra for ML
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../appendix_1_-_linear_algebra_for_ml/2021-03-15-eigen.html">
   Eigen values
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../appendix_1_-_linear_algebra_for_ml/2021-03-15-determinant.html">
   Determinant
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../appendix_1_-_linear_algebra_for_ml/2021-03-15-Positive-semi-definite.html">
   Positive definiteness
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../appendix_1_-_linear_algebra_for_ml/2022-02-11-matrix.html">
   Matrix as transformation and interpreting low rank matrix
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Appendix 2 - Stochastic processes
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../appendix_2_-_stochastic_processes/2021-03-19-stochastic-processes.html">
   Stochastic processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../appendix_2_-_stochastic_processes/2021-03-17-Stationary-Time_Series.html">
   Stationarity of time-series stochastic process
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../appendix_2_-_stochastic_processes/2021-03-23-Stationarity-stochastic-processes.html">
   Stationarity of stochastic processes II
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../references/references.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/notebooks/ml_softwares/2017-08-12-linear-regression-adagrad-vs-gd.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/prog-ml/prog-ml.github.io"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/prog-ml/prog-ml.github.io/issues/new?title=Issue%20on%20page%20%2Fnotebooks/ml_softwares/2017-08-12-linear-regression-adagrad-vs-gd.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/prog-ml/prog-ml.github.io/edit/main/notebooks/ml_softwares/2017-08-12-linear-regression-adagrad-vs-gd.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/prog-ml/prog-ml.github.io/main?urlpath=tree/notebooks/ml_softwares/2017-08-12-linear-regression-adagrad-vs-gd.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/prog-ml/prog-ml.github.io/blob/main/notebooks/ml_softwares/2017-08-12-linear-regression-adagrad-vs-gd.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#formulation-borrowed-from-here-http-ruder-io-optimizing-gradient-descent">
   Formulation ([borrowed from here])((http://ruder.io/optimizing-gradient-descent/)))
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#customary-imports">
   Customary imports
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#true-model">
   True model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#generating-data">
   Generating data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-to-be-learnt">
   Model to be learnt
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#defining-the-cost-function">
   Defining the cost function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dry-run-of-cost-and-gradient-functioning">
   Dry run of cost and gradient functioning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#adagrad-algorithm-applied-on-whole-data-batch">
   Adagrad algorithm (applied on whole data batch)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#experiment-time">
   Experiment time!
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#evolution-of-learning-rates-for-w-and-b">
     Evolution of learning rates for
     <code class="docutils literal notranslate">
      <span class="pre">
       W
      </span>
     </code>
     and
     <code class="docutils literal notranslate">
      <span class="pre">
       b
      </span>
     </code>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gd-vs-adagrad">
     GD vs Adagrad!
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#evolution-of-w-and-b-wrt-eta-and-epsilon">
     Evolution of
     <code class="docutils literal notranslate">
      <span class="pre">
       W
      </span>
     </code>
     and
     <code class="docutils literal notranslate">
      <span class="pre">
       b
      </span>
     </code>
     , wrt
     <span class="math notranslate nohighlight">
      \(\eta\)
     </span>
     and
     <span class="math notranslate nohighlight">
      \(\epsilon\)
     </span>
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#plotting-cost-v-s-iterations">
       Plotting cost v/s # Iterations
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#w-v-s-iterations">
       <code class="docutils literal notranslate">
        <span class="pre">
         W
        </span>
       </code>
       v/s # Iterations
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#b-v-s-iterations">
       <code class="docutils literal notranslate">
        <span class="pre">
         b
        </span>
       </code>
       v/s # Iterations
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualising-the-model-learning">
     Visualising the model learning
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Programatically understanding Adagrad</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#formulation-borrowed-from-here-http-ruder-io-optimizing-gradient-descent">
   Formulation ([borrowed from here])((http://ruder.io/optimizing-gradient-descent/)))
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#customary-imports">
   Customary imports
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#true-model">
   True model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#generating-data">
   Generating data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-to-be-learnt">
   Model to be learnt
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#defining-the-cost-function">
   Defining the cost function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dry-run-of-cost-and-gradient-functioning">
   Dry run of cost and gradient functioning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#adagrad-algorithm-applied-on-whole-data-batch">
   Adagrad algorithm (applied on whole data batch)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#experiment-time">
   Experiment time!
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#evolution-of-learning-rates-for-w-and-b">
     Evolution of learning rates for
     <code class="docutils literal notranslate">
      <span class="pre">
       W
      </span>
     </code>
     and
     <code class="docutils literal notranslate">
      <span class="pre">
       b
      </span>
     </code>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gd-vs-adagrad">
     GD vs Adagrad!
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#evolution-of-w-and-b-wrt-eta-and-epsilon">
     Evolution of
     <code class="docutils literal notranslate">
      <span class="pre">
       W
      </span>
     </code>
     and
     <code class="docutils literal notranslate">
      <span class="pre">
       b
      </span>
     </code>
     , wrt
     <span class="math notranslate nohighlight">
      \(\eta\)
     </span>
     and
     <span class="math notranslate nohighlight">
      \(\epsilon\)
     </span>
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#plotting-cost-v-s-iterations">
       Plotting cost v/s # Iterations
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#w-v-s-iterations">
       <code class="docutils literal notranslate">
        <span class="pre">
         W
        </span>
       </code>
       v/s # Iterations
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#b-v-s-iterations">
       <code class="docutils literal notranslate">
        <span class="pre">
         b
        </span>
       </code>
       v/s # Iterations
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualising-the-model-learning">
     Visualising the model learning
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="programatically-understanding-adagrad">
<h1>Programatically understanding Adagrad<a class="headerlink" href="#programatically-understanding-adagrad" title="Permalink to this headline">Â¶</a></h1>
<blockquote>
<div><p>Or Adagrad for linear regression from basics</p>
</div></blockquote>
<ul class="simple">
<li><p>toc: true</p></li>
<li><p>badges: true</p></li>
<li><p>comments: true</p></li>
<li><p>author: Nipun Batra</p></li>
<li><p>categories: [ML]</p></li>
</ul>
<p>In this post, Iâ€™ll be using <a class="reference external" href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">Adagrad</a> for solving linear regression. As usual, the purpose of this post is educational. <a class="reference external" href="http://ruder.io/optimizing-gradient-descent/">This link</a> gives a good overview of Adagrad alongwith other variants of Gradient Descent. To summarise from the link:</p>
<blockquote>
<div><p>It adapts the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequent parameters. For this reason, it is well-suited for dealing with sparse data.</p>
</div></blockquote>
<p>As Iâ€™d done previously, Iâ€™ll be using <a class="reference external" href="https://github.com/HIPS/autograd">Autograd</a> to compute the gradients. Please note Autograd and not Adagrad!</p>
<div class="section" id="formulation-borrowed-from-here-http-ruder-io-optimizing-gradient-descent">
<h2>Formulation ([borrowed from here])((<a class="reference external" href="http://ruder.io/optimizing-gradient-descent/">http://ruder.io/optimizing-gradient-descent/</a>)))<a class="headerlink" href="#formulation-borrowed-from-here-http-ruder-io-optimizing-gradient-descent" title="Permalink to this headline">Â¶</a></h2>
<p>In regular gradient descent, we would update the <span class="math notranslate nohighlight">\(i^{th}\)</span> parameter in the <span class="math notranslate nohighlight">\(t+1^{th}\)</span> iteration, given the learning rate <span class="math notranslate nohighlight">\(\eta\)</span>, where <span class="math notranslate nohighlight">\(g_{t, i}\)</span> represents the gradient of the cost wrt <span class="math notranslate nohighlight">\(i^{th}\)</span> param at time <span class="math notranslate nohighlight">\(t\)</span>.</p>
<div class="math notranslate nohighlight">
\[ \theta_{t+1, i} = \theta_{t, i} - \eta \cdot g_{t, i}  \tag{Eq 1} \]</div>
<p>In Adagrad, we update as follows:</p>
<div class="math notranslate nohighlight">
\[\theta_{t+1, i} = \theta_{t, i} - \dfrac{\eta}{\sqrt{G_{t, ii} + \epsilon}} \cdot g_{t, i} \tag{Eq 2}\]</div>
<p>Here,</p>
<p><span class="math notranslate nohighlight">\(G_{t} \in \mathbb{R}^{d \times d}\)</span> is a diagonal matrix where each diagonal element <span class="math notranslate nohighlight">\(i, i\)</span> is the sum of the squares of the gradients w.r.t. <span class="math notranslate nohighlight">\(\theta_i\)</span> up to time step <span class="math notranslate nohighlight">\(t\)</span> , while <span class="math notranslate nohighlight">\(\epsilon\)</span> is a smoothing term that avoids division by zero (usually on the order of 1eâˆ’8).</p>
</div>
<div class="section" id="customary-imports">
<h2>Customary imports<a class="headerlink" href="#customary-imports" title="Permalink to this headline">Â¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">autograd.numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="true-model">
<h2>True model<a class="headerlink" href="#true-model" title="Permalink to this headline">Â¶</a></h2>
<div class="math notranslate nohighlight">
\[Y = 10 X + 6\]</div>
</div>
<div class="section" id="generating-data">
<h2>Generating data<a class="headerlink" href="#generating-data" title="Permalink to this headline">Â¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="mi">10</span><span class="o">*</span><span class="n">X</span> <span class="o">+</span> <span class="mi">6</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="s1">&#39;k.&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Y&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/2017-08-12-linear-regression-adagrad-vs-gd_7_0.png" src="../../_images/2017-08-12-linear-regression-adagrad-vs-gd_7_0.png" />
</div>
</div>
</div>
<div class="section" id="model-to-be-learnt">
<h2>Model to be learnt<a class="headerlink" href="#model-to-be-learnt" title="Permalink to this headline">Â¶</a></h2>
<p>We want to learn <code class="docutils literal notranslate"><span class="pre">W</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code> such that:</p>
<div class="math notranslate nohighlight">
\[Y = 10 W+ b\]</div>
</div>
<div class="section" id="defining-the-cost-function">
<h2>Defining the cost function<a class="headerlink" href="#defining-the-cost-function" title="Permalink to this headline">Â¶</a></h2>
<p>We will now write a general cost function that accepts a list of parameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">cost</span><span class="p">(</span><span class="n">param_list</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">param_list</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">w</span><span class="o">*</span><span class="n">X</span><span class="o">+</span><span class="n">b</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(((</span><span class="n">pred</span> <span class="o">-</span> <span class="n">Y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">Y</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="dry-run-of-cost-and-gradient-functioning">
<h2>Dry run of cost and gradient functioning<a class="headerlink" href="#dry-run-of-cost-and-gradient-functioning" title="Permalink to this headline">Â¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Cost of w=0, b=0</span>
<span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cost at w=</span><span class="si">{}</span><span class="s2">, b=</span><span class="si">{}</span><span class="s2"> is: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">cost</span><span class="p">([</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">])))</span>

<span class="c1"># Cost of w=10, b=4. Should be lower than w=0, b=0</span>
<span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">4.</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cost at w=</span><span class="si">{}</span><span class="s2">, b=</span><span class="si">{}</span><span class="s2"> is: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">cost</span><span class="p">([</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">])))</span>

<span class="c1"># Computing the gradient at w=0, b=0</span>
<span class="kn">from</span> <span class="nn">autograd</span> <span class="kn">import</span> <span class="n">grad</span>
<span class="n">grad_cost</span> <span class="o">=</span><span class="n">grad</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
<span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Gradient at w=</span><span class="si">{}</span><span class="s2">, b=</span><span class="si">{}</span><span class="s2"> is: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">grad_cost</span><span class="p">([</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">])))</span>

<span class="c1"># Computing the gradient at w=10, b=4. We would expect it to be smaller than at 0, 0</span>
<span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">4.</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Gradient at w=</span><span class="si">{}</span><span class="s2">, b=</span><span class="si">{}</span><span class="s2"> is: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">grad_cost</span><span class="p">([</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">])))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost at w=0.0, b=0.0 is: 2.98090446495
Cost at w=10.0, b=4.0 is: 0.0320479471939
Gradient at w=0.0, b=0.0 is: [array(-0.29297046699711365), array(-0.008765162440358071)]
Gradient at w=10.0, b=4.0 is: [array(-0.14406455246023858), array(-0.007117830452061141)]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="adagrad-algorithm-applied-on-whole-data-batch">
<h2>Adagrad algorithm (applied on whole data batch)<a class="headerlink" href="#adagrad-algorithm-applied-on-whole-data-batch" title="Permalink to this headline">Â¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">adagrad_gd</span><span class="p">(</span><span class="n">param_init</span><span class="p">,</span> <span class="n">cost</span><span class="p">,</span> <span class="n">niter</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    param_init: List of initial values of parameters</span>
<span class="sd">    cost: cost function</span>
<span class="sd">    niter: Number of iterations to run</span>
<span class="sd">    lr: Learning rate</span>
<span class="sd">    eps: Fudge factor, to avoid division by zero</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">copy</span> <span class="kn">import</span> <span class="n">deepcopy</span>
    <span class="kn">import</span> <span class="nn">math</span>
    <span class="c1"># Fixing the random_seed</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">random_seed</span><span class="p">)</span>
    
    <span class="c1"># Function to compute the gradient of the cost function</span>
    <span class="n">grad_cost</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">param_init</span><span class="p">)</span>
    <span class="n">param_array</span><span class="p">,</span> <span class="n">grad_array</span><span class="p">,</span> <span class="n">lr_array</span><span class="p">,</span> <span class="n">cost_array</span> <span class="o">=</span> <span class="p">[</span><span class="n">params</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[[</span><span class="n">lr</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">params</span><span class="p">]],</span> <span class="p">[</span><span class="n">cost</span><span class="p">(</span><span class="n">params</span><span class="p">)]</span>
    <span class="c1"># Initialising sum of squares of gradients for each param as 0</span>
    <span class="n">sum_squares_gradients</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">param</span><span class="p">)</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">params</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">niter</span><span class="p">):</span>
        <span class="n">out_params</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">gradients</span> <span class="o">=</span> <span class="n">grad_cost</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="c1"># At each iteration, we add the square of the gradients to `sum_squares_gradients`</span>
        <span class="n">sum_squares_gradients</span><span class="o">=</span> <span class="p">[</span><span class="n">eps</span> <span class="o">+</span> <span class="n">sum_prev</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">g</span><span class="p">)</span> <span class="k">for</span> <span class="n">sum_prev</span><span class="p">,</span> <span class="n">g</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">sum_squares_gradients</span><span class="p">,</span> <span class="n">gradients</span><span class="p">)]</span>
        <span class="c1"># Adapted learning rate for parameter list</span>
        <span class="n">lrs</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sg</span><span class="p">))</span> <span class="k">for</span> <span class="n">sg</span> <span class="ow">in</span> <span class="n">sum_squares_gradients</span><span class="p">]</span>
        <span class="c1"># Paramter update</span>
        <span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">param</span><span class="o">-</span><span class="p">(</span><span class="n">adapted_lr</span><span class="o">*</span><span class="n">grad_param</span><span class="p">)</span> <span class="k">for</span> <span class="n">param</span><span class="p">,</span> <span class="n">adapted_lr</span><span class="p">,</span> <span class="n">grad_param</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lrs</span><span class="p">,</span> <span class="n">gradients</span><span class="p">)]</span>
        <span class="n">param_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="n">lr_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lrs</span><span class="p">)</span>
        <span class="n">grad_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gradients</span><span class="p">)</span>
        <span class="n">cost_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">(</span><span class="n">params</span><span class="p">))</span>
        
    <span class="k">return</span> <span class="n">params</span><span class="p">,</span> <span class="n">param_array</span><span class="p">,</span> <span class="n">grad_array</span><span class="p">,</span> <span class="n">lr_array</span><span class="p">,</span> <span class="n">cost_array</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="experiment-time">
<h2>Experiment time!<a class="headerlink" href="#experiment-time" title="Permalink to this headline">Â¶</a></h2>
<div class="section" id="evolution-of-learning-rates-for-w-and-b">
<h3>Evolution of learning rates for <code class="docutils literal notranslate"><span class="pre">W</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code><a class="headerlink" href="#evolution-of-learning-rates-for-w-and-b" title="Permalink to this headline">Â¶</a></h3>
<p>Let us see how the learning rate for <code class="docutils literal notranslate"><span class="pre">W</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code> will evolve over time. I will fix the initial learning rate to 0.01 as mot of the Adagrad literature out there seems to suggest.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fixing the random seed for reproducible init params for `W` and `b`</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">param_init</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(),</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()]</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span>
<span class="n">niter</span><span class="o">=</span><span class="mi">1000</span>
<span class="n">ada_params</span><span class="p">,</span> <span class="n">ada_param_array</span><span class="p">,</span> <span class="n">ada_grad_array</span><span class="p">,</span> <span class="n">ada_lr_array</span><span class="p">,</span> <span class="n">ada_cost_array</span> <span class="o">=</span> <span class="n">adagrad_gd</span><span class="p">(</span><span class="n">param_init</span><span class="p">,</span> <span class="n">cost</span><span class="p">,</span> <span class="n">niter</span><span class="o">=</span><span class="n">niter</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let us first see the evolution of cost wrt time</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">ada_cost_array</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Cost&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;Adagrad: Cost v/s # Iterations&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Cost&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;# Iterations&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/2017-08-12-linear-regression-adagrad-vs-gd_21_0.png" src="../../_images/2017-08-12-linear-regression-adagrad-vs-gd_21_0.png" />
</div>
</div>
<p>Ok. While There seems to be a drop in the cost, the converegence will be very slow. Remember that we had earlier found</p>
<blockquote>
<div><p>Cost at w=10.0, b=4.0 is: 0.0320479471939</p>
</div></blockquote>
<p>Iâ€™m sure this means that our parameter estimates are similar to the initial parameters and far from the true parameters. Letâ€™s just confirm the same.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;After </span><span class="si">{}</span><span class="s2"> iterations, learnt `W` = </span><span class="si">{}</span><span class="s2"> and learnt `b` = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">niter</span><span class="p">,</span> <span class="o">*</span><span class="n">ada_params</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>After 1000 iterations, learnt `W` = 2.38206194526 and learnt `b` = 1.01811878873
</pre></div>
</div>
</div>
</div>
<p>I would suspect that the learning rate, courtesy of the adaptive nature is falling very rapidly! How would the vanilla gradient descent have done starting with the same learning rate and initial values? My hunch is it would do better. Letâ€™s  confirm!</p>
</div>
<div class="section" id="gd-vs-adagrad">
<h3>GD vs Adagrad!<a class="headerlink" href="#gd-vs-adagrad" title="Permalink to this headline">Â¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gd</span><span class="p">(</span><span class="n">param_init</span><span class="p">,</span> <span class="n">cost</span><span class="p">,</span>  <span class="n">niter</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">random_seed</span><span class="p">)</span>
    <span class="kn">from</span> <span class="nn">copy</span> <span class="kn">import</span> <span class="n">deepcopy</span>
    <span class="n">grad_cost</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">param_init</span><span class="p">)</span>
    <span class="n">param_array</span><span class="p">,</span> <span class="n">grad_array</span><span class="p">,</span> <span class="n">cost_array</span> <span class="o">=</span> <span class="p">[</span><span class="n">params</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="n">cost</span><span class="p">(</span><span class="n">params</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">niter</span><span class="p">):</span>
        <span class="n">out_params</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">gradients</span> <span class="o">=</span> <span class="n">grad_cost</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">param</span><span class="o">-</span><span class="n">lr</span><span class="o">*</span><span class="n">grad_param</span> <span class="k">for</span> <span class="n">param</span><span class="p">,</span> <span class="n">grad_param</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">gradients</span><span class="p">)]</span>
        <span class="n">param_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="n">grad_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gradients</span><span class="p">)</span>
        <span class="n">cost_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">(</span><span class="n">params</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">params</span><span class="p">,</span> <span class="n">param_array</span><span class="p">,</span> <span class="n">grad_array</span><span class="p">,</span> <span class="n">cost_array</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fixing the random seed for reproducible init params for `W` and `b`</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">param_init</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(),</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()]</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">niter</span><span class="o">=</span><span class="mi">1000</span>
<span class="n">gd_params</span><span class="p">,</span> <span class="n">gd_param_array</span><span class="p">,</span> <span class="n">gd_grad_array</span><span class="p">,</span> <span class="n">gd_cost</span> <span class="o">=</span> <span class="n">gd</span><span class="p">(</span><span class="n">param_init</span><span class="p">,</span> <span class="n">cost</span><span class="p">,</span> <span class="n">niter</span><span class="o">=</span><span class="n">niter</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">ada_cost_array</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Cost&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Adagrad&#39;</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">gd_cost</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Cost&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;GD&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Cost&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;# Iterations&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x1153b4ad0&gt;
</pre></div>
</div>
<img alt="../../_images/2017-08-12-linear-regression-adagrad-vs-gd_28_1.png" src="../../_images/2017-08-12-linear-regression-adagrad-vs-gd_28_1.png" />
</div>
</div>
<p>Ok. So, indeed with learning rate of 0.01, gradient descent fares better. Letâ€™s just confirm that for Adagrad, the learning rates diminish rapidly leading to little reduction in cost!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">ada_lr_array</span><span class="p">),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;LR for W&#39;</span><span class="p">,</span> <span class="s1">&#39;LR for b&#39;</span><span class="p">])[::</span><span class="mi">50</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">subplots</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;# Iterations&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.text.Text at 0x11569c4d0&gt;
</pre></div>
</div>
<img alt="../../_images/2017-08-12-linear-regression-adagrad-vs-gd_30_1.png" src="../../_images/2017-08-12-linear-regression-adagrad-vs-gd_30_1.png" />
</div>
</div>
<p>There are a couple of interesting observations:</p>
<ol class="simple">
<li><p>The learning rate for <code class="docutils literal notranslate"><span class="pre">b</span></code> actually increases from its initial value of 0.01. Even after 1000 iterations, it remains more than its initial value. This can be explained by the fact that the suim of squares gradients wrt <code class="docutils literal notranslate"><span class="pre">b</span></code> would be less than 1. Thus, the denominator term by which the learning rate gets divided will be less than 1. Thus, increasing the learning rate wrt b. This can however be fixed by choosing <span class="math notranslate nohighlight">\(\epsilon=1.0\)</span></p></li>
<li><p>The learning rate for <code class="docutils literal notranslate"><span class="pre">W</span></code> falls very rapidly. Learning would be negligble for <code class="docutils literal notranslate"><span class="pre">W</span></code> after the initial few iterations. This can be fixed by choosing a larger initial learning rate <span class="math notranslate nohighlight">\(\eta\)</span>.</p></li>
</ol>
</div>
<div class="section" id="evolution-of-w-and-b-wrt-eta-and-epsilon">
<h3>Evolution of <code class="docutils literal notranslate"><span class="pre">W</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code>, wrt <span class="math notranslate nohighlight">\(\eta\)</span> and <span class="math notranslate nohighlight">\(\epsilon\)</span><a class="headerlink" href="#evolution-of-w-and-b-wrt-eta-and-epsilon" title="Permalink to this headline">Â¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fixing the random seed for reproducible init params for `W` and `b`</span>
<span class="n">out</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">lr</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">]:</span>
    <span class="n">out</span><span class="p">[</span><span class="n">lr</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">eps</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">1e-8</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">param_init</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(),</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()]</span>
        <span class="n">niter</span><span class="o">=</span><span class="mi">10000</span>
        <span class="n">ada_params</span><span class="p">,</span> <span class="n">ada_param_array</span><span class="p">,</span> <span class="n">ada_grad_array</span><span class="p">,</span> <span class="n">ada_lr_array</span><span class="p">,</span> <span class="n">ada_cost_array</span> <span class="o">=</span> <span class="n">adagrad_gd</span><span class="p">(</span><span class="n">param_init</span><span class="p">,</span>
                                                                                               <span class="n">cost</span><span class="p">,</span> 
                                                                                               <span class="n">niter</span><span class="o">=</span><span class="n">niter</span><span class="p">,</span>
                                                                                               <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> 
                                                                                               <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
        <span class="n">out</span><span class="p">[</span><span class="n">lr</span><span class="p">][</span><span class="n">eps</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;Final-params&#39;</span><span class="p">:</span><span class="n">ada_params</span><span class="p">,</span>
                       <span class="s1">&#39;Param-array&#39;</span><span class="p">:</span><span class="n">ada_param_array</span><span class="p">,</span>
                       <span class="s1">&#39;Cost-array&#39;</span><span class="p">:</span><span class="n">ada_cost_array</span><span class="p">}</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(0.01, 1e-08)
(0.01, 0.1)
(0.01, 1)
(0.1, 1e-08)
(0.1, 0.1)
(0.1, 1)
(1, 1e-08)
(1, 0.1)
(1, 1)
(10, 1e-08)
(10, 0.1)
(10, 1)
</pre></div>
</div>
</div>
</div>
<div class="section" id="plotting-cost-v-s-iterations">
<h4>Plotting cost v/s # Iterations<a class="headerlink" href="#plotting-cost-v-s-iterations" title="Permalink to this headline">Â¶</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">row</span><span class="p">,</span> <span class="n">eps</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mf">1e-8</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">column</span><span class="p">,</span> <span class="n">lr</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">]):</span>
        <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="n">lr</span><span class="p">][</span><span class="n">eps</span><span class="p">][</span><span class="s1">&#39;Cost-array&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">column</span><span class="p">])</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">column</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Eta=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr</span><span class="p">))</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Eps=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">eps</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="s1">&#39;# Iterations&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Cost v/s # Iterations&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/2017-08-12-linear-regression-adagrad-vs-gd_35_0.png" src="../../_images/2017-08-12-linear-regression-adagrad-vs-gd_35_0.png" />
</div>
</div>
<p>It seems that choosing <span class="math notranslate nohighlight">\(\eta=1\)</span> or above the cost usually converges quickly. This seems to be different from most literature recommending <span class="math notranslate nohighlight">\(\eta=0.01\)</span>. Aside: I confirmed that even using Tensorflow on the same dataset with Adagrad optimizer, the optimal learning rates are similar to the ones we found here!</p>
</div>
<div class="section" id="w-v-s-iterations">
<h4><code class="docutils literal notranslate"><span class="pre">W</span></code> v/s # Iterations<a class="headerlink" href="#w-v-s-iterations" title="Permalink to this headline">Â¶</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">row</span><span class="p">,</span> <span class="n">eps</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mf">1e-8</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">column</span><span class="p">,</span> <span class="n">lr</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">]):</span>
        <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="n">lr</span><span class="p">][</span><span class="n">eps</span><span class="p">][</span><span class="s1">&#39;Param-array&#39;</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">column</span><span class="p">])</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">column</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Eta=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr</span><span class="p">))</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Eps=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">eps</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="s1">&#39;# Iterations&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;W v/s # Iterations&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/2017-08-12-linear-regression-adagrad-vs-gd_38_0.png" src="../../_images/2017-08-12-linear-regression-adagrad-vs-gd_38_0.png" />
</div>
</div>
</div>
<div class="section" id="b-v-s-iterations">
<h4><code class="docutils literal notranslate"><span class="pre">b</span></code> v/s # Iterations<a class="headerlink" href="#b-v-s-iterations" title="Permalink to this headline">Â¶</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">row</span><span class="p">,</span> <span class="n">eps</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mf">1e-8</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">column</span><span class="p">,</span> <span class="n">lr</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">]):</span>
        <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="n">lr</span><span class="p">][</span><span class="n">eps</span><span class="p">][</span><span class="s1">&#39;Param-array&#39;</span><span class="p">])[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">column</span><span class="p">])</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">column</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Eta=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr</span><span class="p">))</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Eps=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">eps</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="s1">&#39;# Iterations&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;b v/s # Iterations&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/2017-08-12-linear-regression-adagrad-vs-gd_40_0.png" src="../../_images/2017-08-12-linear-regression-adagrad-vs-gd_40_0.png" />
</div>
</div>
<p>Across the above two plots, we can see that at high <span class="math notranslate nohighlight">\(\eta\)</span>, there are oscillations! In general, <span class="math notranslate nohighlight">\(\eta=1\)</span> and <span class="math notranslate nohighlight">\(\epsilon=1e-8\)</span> seem to give the best set of results.</p>
</div>
</div>
<div class="section" id="visualising-the-model-learning">
<h3>Visualising the model learning<a class="headerlink" href="#visualising-the-model-learning" title="Permalink to this headline">Â¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib.animation</span> <span class="kn">import</span> <span class="n">FuncAnimation</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
    <span class="c1">#fig.clf()</span>
    <span class="k">for</span> <span class="n">row</span><span class="p">,</span> <span class="n">eps</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mf">1e-8</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">column</span><span class="p">,</span> <span class="n">lr</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">]):</span>
            <span class="n">params_i</span> <span class="o">=</span>  <span class="n">out</span><span class="p">[</span><span class="n">lr</span><span class="p">][</span><span class="n">eps</span><span class="p">][</span><span class="s1">&#39;Param-array&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">column</span><span class="p">]</span><span class="o">.</span><span class="n">cla</span><span class="p">()</span>
            <span class="n">w_i</span><span class="p">,</span> <span class="n">b_i</span> <span class="o">=</span> <span class="n">params_i</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">column</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="s1">&#39;k.&#39;</span><span class="p">,</span> <span class="n">ms</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">column</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w_i</span><span class="o">*</span><span class="n">X</span><span class="o">+</span><span class="n">b_i</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">column</span><span class="p">]</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span> <span class="c1">#https://stackoverflow.com/questions/12998430/remove-xticks-in-a-matplotlib-plot</span>
                <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;both&#39;</span><span class="p">,</span>         
                <span class="n">which</span><span class="o">=</span><span class="s1">&#39;both&#39;</span><span class="p">,</span>      
                <span class="n">bottom</span><span class="o">=</span><span class="s1">&#39;off&#39;</span><span class="p">,</span> 
                <span class="n">left</span><span class="o">=</span><span class="s1">&#39;off&#39;</span><span class="p">,</span>
                <span class="n">top</span><span class="o">=</span><span class="s1">&#39;off&#39;</span><span class="p">,</span>         
                <span class="n">labelbottom</span><span class="o">=</span><span class="s1">&#39;off&#39;</span><span class="p">,</span>
                <span class="n">labelleft</span><span class="o">=</span><span class="s1">&#39;off&#39;</span><span class="p">)</span> 
            <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">column</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Eta=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr</span><span class="p">))</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Eps=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">eps</span><span class="p">))</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Iteration number: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>

<span class="n">anim</span> <span class="o">=</span> <span class="n">FuncAnimation</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">update</span><span class="p">,</span> <span class="n">frames</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5000</span><span class="p">,</span> <span class="mi">200</span><span class="p">),</span> <span class="n">interval</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="n">anim</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;adagrad.gif&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">writer</span><span class="o">=</span><span class="s1">&#39;imagemagick&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p><img alt="" src="https://nipunbatra.github.io/blog/images/adagrad.gif" /></p>
<p>So, there you go. Implementing Adagrad and running this experiment was a lot of fun and learning. Feel free to comment!</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "prog-ml/prog-ml.github.io",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks/ml_softwares"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="2022-02-09-autograd-pytorch-jax.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Autograd in JAX and PyTorch</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../others/2021-03-24-sigmoid.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">&lt;no title&gt;</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By The Jupyter Book community<br/>
    
        &copy; Copyright 2022.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>
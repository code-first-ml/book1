
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Understanding KL-Divergence &#8212; Code-First-ML</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="&lt;no title&gt;" href="jensen-inequality.html" />
    <link rel="prev" title="GMM learning" href="../mixture_models/2022-02-14-GMM.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/code-first-ml.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Code-First-ML</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Code-First-ML
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/mle_coin.html">
   MLE, MAP and Fully Bayesian (conjugate prior and MCMC) for coin toss
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/variational.html">
   Variational Inference from scratch in JAX
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Probability - Univariate Models
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../probability/Sample_Space_and_Random_Variables.html">
   Sample Space and Random Variables
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../probability/univariate-normal.html">
   Properties of RV
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../probability/univariate-normal-expectations.html">
   Derivations for moments of univariate normal distribution
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Probability - Multivariate Models
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../probability/mvn-introduction.html">
   Multivariate Normal Distribution: Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../probability/mvn-marginal.html">
   Multivariate Normal Distribution: Marginals
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Statistics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml/2021-03-23-bayesian-ml.html">
   Bayesian ML: Fundamentals
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml/2021-04-14-bayesian-linear-regression.html">
   Bayesian linear regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml/2021-03-29-bayesian-model-selection.html">
   Bayesian model selection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml/2021-03-27-Marginal-Likelihood-2.html">
   Marginal likelihood
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml/2021-03-31-derivation-of-marginal-likelihood.html">
   Marginal likelihood for Bayesian linear regression
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Decision Theory
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../sampling/rejection-sampling-lr.html">
   Simple rejection sampling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../sampling/Metropolis-Hastings.html">
   Metropolis Hastings
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../sampling/2021-03-10-importance-sampling.html">
   Importance sampling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../sampling/2021-03-10-rejection-sampling.html">
   Rejection sampling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../sampling/2022-02-04-sampling-normal.html">
   Sampling from univariate and multivariate normal distributions using Box-Muller transform
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../sampling/2020-04-16-inverse-transform.html">
   Sampling from common distributions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../sampling/2014-05-01-gibbs-sampling.html">
   Gibbs sampling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../sampling/2014-07-01-mcmc_coins.html">
   Coin tosses and MCMC
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Information Theory
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../graphical_models/2022-02-15-draw-graphical-models.html">
   Drawing graphical models
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Linear Algebra
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../mixture_models/2022-02-14-GMM.html">
   GMM learning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Optimization
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Understanding KL-Divergence
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Neural networks for structured data
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../neural_networks/2020-03-08-keras-neural-non-linear.html">
   Some Neural Network Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neural_networks/2020-02-28-xor-relu-vector.html">
   Learning neural network for XOR
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neural_networks/2020-03-02-linear-scratch.html">
   Neural Networks from scratch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neural_networks/2018-01-13-denoising.html">
   Signal denoising using RNNs in PyTorch
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Recommender systems
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../recommender_systems/2017-12-18-recommend-keras.html">
   Recommender Systems in Keras
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../recommender_systems/2017-12-29-neural-collaborative-filtering.html">
   Neural Networks for Collaborative Filtering
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../references/references.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/code-first-ml/book1/main?urlpath=tree/notebooks/information_theory/kl-divergence.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/code-first-ml/book1/blob/main/notebooks/information_theory/kl-divergence.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
      <li>
        
<button onclick="initThebeSBT()"
  class="headerbtn headerbtn-launch-thebe"
  data-toggle="tooltip"
data-placement="left"
title="Launch Thebe"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="headerbtn__text-container">Live Code</span>
</button>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/code-first-ml/book1"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/code-first-ml/book1/issues/new?title=Issue%20on%20page%20%2Fnotebooks/information_theory/kl-divergence.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/code-first-ml/book1/edit/main/notebooks/information_theory/kl-divergence.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/notebooks/information_theory/kl-divergence.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#goals">
   Goals:
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#g1-given-probability-distributions-p-and-q-find-the-divergence-measure-of-similarity-between-them">
     G1: Given probability distributions
     <span class="math notranslate nohighlight">
      \(p\)
     </span>
     and
     <span class="math notranslate nohighlight">
      \(q\)
     </span>
     , find the divergence (measure of similarity) between them
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#g2-assuming-p-to-be-fixed-can-we-find-optimum-parameters-of-q-to-make-it-as-close-as-possible-to-p">
     G2: assuming
     <span class="math notranslate nohighlight">
      \(p\)
     </span>
     to be fixed, can we find optimum parameters of
     <span class="math notranslate nohighlight">
      \(q\)
     </span>
     to make it as close as possible to
     <span class="math notranslate nohighlight">
      \(p\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#g3-finding-the-distance-between-two-distributions-of-different-families">
     G3: finding the “distance” between two distributions of different families
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#g4-optimizing-the-distance-between-two-distributions-of-different-families">
     G4: optimizing the “distance” between two distributions of different families
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#g5-approximating-the-kl-divergence">
     G5: Approximating the KL-divergence
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#g6-implementing-variational-inference-for-linear-regression">
     G6: Implementing variational inference for linear regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#basic-imports">
   Basic Imports
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#creating-distributions">
   Creating distributions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#creating-p-sim-mathcal-n-1-00-4-00">
     Creating
     <span class="math notranslate nohighlight">
      \(p\sim\mathcal{N}(1.00, 4.00)\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#creating-q-sim-mathcal-n-loc-scale">
     Creating
     <span class="math notranslate nohighlight">
      \(q\sim\mathcal{N}(loc, scale)\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generating-a-few-qs-for-different-location-and-scale-value">
     Generating a few qs for different location and scale value
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimizing-the-kl-divergence-between-q-and-p">
   Optimizing the KL-divergence between q and p
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#animation">
   Animation!
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#finding-the-kl-divergence-for-two-distributions-from-different-families">
   Finding the KL divergence for two distributions from different families
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimizing-the-kl-divergence-for-two-distributions-from-different-families">
   Optimizing the KL divergence for two distributions from different families
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimizing-the-kl-divergence-between-two-2d-distributions">
   Optimizing the KL-divergence between two 2d distributions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#to-fix">
   To-FIX
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kl-divergence-and-elbo">
     KL-Divergence and ELBO
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Understanding KL-Divergence</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#goals">
   Goals:
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#g1-given-probability-distributions-p-and-q-find-the-divergence-measure-of-similarity-between-them">
     G1: Given probability distributions
     <span class="math notranslate nohighlight">
      \(p\)
     </span>
     and
     <span class="math notranslate nohighlight">
      \(q\)
     </span>
     , find the divergence (measure of similarity) between them
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#g2-assuming-p-to-be-fixed-can-we-find-optimum-parameters-of-q-to-make-it-as-close-as-possible-to-p">
     G2: assuming
     <span class="math notranslate nohighlight">
      \(p\)
     </span>
     to be fixed, can we find optimum parameters of
     <span class="math notranslate nohighlight">
      \(q\)
     </span>
     to make it as close as possible to
     <span class="math notranslate nohighlight">
      \(p\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#g3-finding-the-distance-between-two-distributions-of-different-families">
     G3: finding the “distance” between two distributions of different families
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#g4-optimizing-the-distance-between-two-distributions-of-different-families">
     G4: optimizing the “distance” between two distributions of different families
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#g5-approximating-the-kl-divergence">
     G5: Approximating the KL-divergence
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#g6-implementing-variational-inference-for-linear-regression">
     G6: Implementing variational inference for linear regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#basic-imports">
   Basic Imports
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#creating-distributions">
   Creating distributions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#creating-p-sim-mathcal-n-1-00-4-00">
     Creating
     <span class="math notranslate nohighlight">
      \(p\sim\mathcal{N}(1.00, 4.00)\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#creating-q-sim-mathcal-n-loc-scale">
     Creating
     <span class="math notranslate nohighlight">
      \(q\sim\mathcal{N}(loc, scale)\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generating-a-few-qs-for-different-location-and-scale-value">
     Generating a few qs for different location and scale value
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimizing-the-kl-divergence-between-q-and-p">
   Optimizing the KL-divergence between q and p
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#animation">
   Animation!
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#finding-the-kl-divergence-for-two-distributions-from-different-families">
   Finding the KL divergence for two distributions from different families
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimizing-the-kl-divergence-for-two-distributions-from-different-families">
   Optimizing the KL divergence for two distributions from different families
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimizing-the-kl-divergence-between-two-2d-distributions">
   Optimizing the KL-divergence between two 2d distributions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#to-fix">
   To-FIX
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kl-divergence-and-elbo">
     KL-Divergence and ELBO
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="understanding-kl-divergence">
<h1>Understanding KL-Divergence<a class="headerlink" href="#understanding-kl-divergence" title="Permalink to this headline">#</a></h1>
<ul class="simple">
<li><p>toc: true</p></li>
<li><p>badges: true</p></li>
<li><p>comments: true</p></li>
<li><p>author: Nipun Batra</p></li>
<li><p>categories: [ML]</p></li>
</ul>
<section id="goals">
<h2>Goals:<a class="headerlink" href="#goals" title="Permalink to this headline">#</a></h2>
<section id="g1-given-probability-distributions-p-and-q-find-the-divergence-measure-of-similarity-between-them">
<h3>G1: Given probability distributions <span class="math notranslate nohighlight">\(p\)</span> and <span class="math notranslate nohighlight">\(q\)</span>, find the divergence (measure of similarity) between them<a class="headerlink" href="#g1-given-probability-distributions-p-and-q-find-the-divergence-measure-of-similarity-between-them" title="Permalink to this headline">#</a></h3>
<p>Let us first look at G1. Look at the illustration below. We have a normal distribution <span class="math notranslate nohighlight">\(p\)</span> and two other normal distributions <span class="math notranslate nohighlight">\(q_1\)</span> and <span class="math notranslate nohighlight">\(q_2\)</span>. Which of <span class="math notranslate nohighlight">\(q_1\)</span> and <span class="math notranslate nohighlight">\(q_2\)</span>, would we consider closer to <span class="math notranslate nohighlight">\(p\)</span>? <span class="math notranslate nohighlight">\(q_2\)</span>, right?</p>
<p><img alt="" src="notebooks/information_theory/dkl.png" /></p>
<p>To understand the notion of similarity, we use a metric called the KL-divergence given as <span class="math notranslate nohighlight">\(D_{KL}(a || b)\)</span> where <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> are the two distributions.</p>
<p>For G1, we can say <span class="math notranslate nohighlight">\(q_2\)</span> is closer to <span class="math notranslate nohighlight">\(p\)</span> compared to <span class="math notranslate nohighlight">\(q_1\)</span> as:</p>
<p><span class="math notranslate nohighlight">\(D_{KL}(q_2 || p) \lt D_{KL}(q_1 || p)\)</span></p>
<p>For the above example, we have the values as <span class="math notranslate nohighlight">\(D_{KL}(q_2|| p) = 0.07\)</span> and <span class="math notranslate nohighlight">\(D_{KL}(q_1|| p)= 0.35\)</span></p>
</section>
<section id="g2-assuming-p-to-be-fixed-can-we-find-optimum-parameters-of-q-to-make-it-as-close-as-possible-to-p">
<h3>G2: assuming <span class="math notranslate nohighlight">\(p\)</span> to be fixed, can we find optimum parameters of <span class="math notranslate nohighlight">\(q\)</span> to make it as close as possible to <span class="math notranslate nohighlight">\(p\)</span><a class="headerlink" href="#g2-assuming-p-to-be-fixed-can-we-find-optimum-parameters-of-q-to-make-it-as-close-as-possible-to-p" title="Permalink to this headline">#</a></h3>
<p>The following GIF shows the process of finding the optimum set of parameters for a normal distribution <span class="math notranslate nohighlight">\(q\)</span> so that it becomes as close as possible to <span class="math notranslate nohighlight">\(p\)</span>. This is equivalent of minimizing <span class="math notranslate nohighlight">\(D_{KL}(q || p)\)</span></p>
<p><img alt="" src="notebooks/information_theory/kl_qp.gif" /></p>
<p>The following GIF shows the above but for a two-dimensional distribution.</p>
<p><img alt="" src="notebooks/information_theory/kl_qp_2.gif" /></p>
</section>
<section id="g3-finding-the-distance-between-two-distributions-of-different-families">
<h3>G3: finding the “distance” between two distributions of different families<a class="headerlink" href="#g3-finding-the-distance-between-two-distributions-of-different-families" title="Permalink to this headline">#</a></h3>
<p>The below image shows the KL-divergence between distribution 1 (mixture of Gaussians) and distribution 2 (Gaussian)</p>
<p><img alt="" src="notebooks/information_theory/dkl-different.png" /></p>
</section>
<section id="g4-optimizing-the-distance-between-two-distributions-of-different-families">
<h3>G4: optimizing the “distance” between two distributions of different families<a class="headerlink" href="#g4-optimizing-the-distance-between-two-distributions-of-different-families" title="Permalink to this headline">#</a></h3>
<p>The below GIF shows the optimization of the KL-divergence between distribution 1 (mixture of Gaussians) and distribution 2 (Gaussian)</p>
<p><img alt="" src="notebooks/information_theory/kl_qp_mg.gif" /></p>
</section>
<section id="g5-approximating-the-kl-divergence">
<h3>G5: Approximating the KL-divergence<a class="headerlink" href="#g5-approximating-the-kl-divergence" title="Permalink to this headline">#</a></h3>
</section>
<section id="g6-implementing-variational-inference-for-linear-regression">
<h3>G6: Implementing variational inference for linear regression<a class="headerlink" href="#g6-implementing-variational-inference-for-linear-regression" title="Permalink to this headline">#</a></h3>
</section>
</section>
<section id="basic-imports">
<h2>Basic Imports<a class="headerlink" href="#basic-imports" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">tensorflow_probability</span> <span class="k">as</span> <span class="nn">tfp</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">tfd</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">distributions</span>
<span class="n">tfl</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">layers</span>
<span class="n">tfb</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">bijectors</span>

<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.optimizers</span> <span class="kn">import</span> <span class="n">RMSprop</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.callbacks</span> <span class="kn">import</span> <span class="n">Callback</span>

<span class="n">sns</span><span class="o">.</span><span class="n">reset_defaults</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">context</span><span class="o">=</span><span class="s2">&quot;talk&quot;</span><span class="p">,</span> <span class="n">font_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format=&#39;retina&#39;
</pre></div>
</div>
</div>
</div>
</section>
<section id="creating-distributions">
<h2>Creating distributions<a class="headerlink" href="#creating-distributions" title="Permalink to this headline">#</a></h2>
<section id="creating-p-sim-mathcal-n-1-00-4-00">
<h3>Creating <span class="math notranslate nohighlight">\(p\sim\mathcal{N}(1.00, 4.00)\)</span><a class="headerlink" href="#creating-p-sim-mathcal-n-1-00-4-00" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-02-04 14:55:14.596076: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">z_values</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">z_values</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">z_values</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">prob_values_p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">prob</span><span class="p">(</span><span class="n">z_values</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_values</span><span class="p">,</span> <span class="n">prob_values_p</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$p\sim\mathcal</span><span class="si">{N}</span><span class="s2">(1.00, 4.00)$&quot;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;PDF&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/kl-divergence_6_0.png" src="../../_images/kl-divergence_6_0.png" />
</div>
</div>
</section>
<section id="creating-q-sim-mathcal-n-loc-scale">
<h3>Creating <span class="math notranslate nohighlight">\(q\sim\mathcal{N}(loc, scale)\)</span><a class="headerlink" href="#creating-q-sim-mathcal-n-loc-scale" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_q</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="generating-a-few-qs-for-different-location-and-scale-value">
<h3>Generating a few qs for different location and scale value<a class="headerlink" href="#generating-a-few-qs-for-different-location-and-scale-value" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">q</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">q</span><span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">create_q</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>

<span class="k">for</span> <span class="n">loc</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]:</span>
    <span class="k">for</span> <span class="n">scale</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]:</span>
        <span class="n">q</span><span class="p">[(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">)]</span> <span class="o">=</span> <span class="n">create_q</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">loc</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="n">scale</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_values</span><span class="p">,</span> <span class="n">prob_values_p</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$p\sim\mathcal</span><span class="si">{N}</span><span class="s2">(1.00, 4.00)$&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">z_values</span><span class="p">,</span>
    <span class="n">create_q</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">)</span><span class="o">.</span><span class="n">prob</span><span class="p">(</span><span class="n">z_values</span><span class="p">),</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$q_1\sim\mathcal</span><span class="si">{N}</span><span class="s2">(0.00, 2.00)$&quot;</span><span class="p">,</span>
    <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">z_values</span><span class="p">,</span>
    <span class="n">create_q</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">)</span><span class="o">.</span><span class="n">prob</span><span class="p">(</span><span class="n">z_values</span><span class="p">),</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$q_2\sim\mathcal</span><span class="si">{N}</span><span class="s2">(1.00, 3.00)$&quot;</span><span class="p">,</span>
    <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;-.&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.04</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">borderaxespad</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;PDF&quot;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span>
    <span class="s2">&quot;dkl.png&quot;</span><span class="p">,</span>
    <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/kl-divergence_11_0.png" src="../../_images/kl-divergence_11_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#### Computing KL-divergence</span>

<span class="n">q_0_2_dkl</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">kl_divergence</span><span class="p">(</span><span class="n">create_q</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">),</span> <span class="n">p</span><span class="p">)</span>
<span class="n">q_1_3_dkl</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">kl_divergence</span><span class="p">(</span><span class="n">create_q</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">),</span> <span class="n">p</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;D_KL (q(0, 2)||p) = </span><span class="si">{</span><span class="n">q_0_2_dkl</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;D_KL (q(1, 3)||p) = </span><span class="si">{</span><span class="n">q_1_3_dkl</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>D_KL (q(0, 2)||p) = 0.35
D_KL (q(1, 3)||p) = 0.07
</pre></div>
</div>
</div>
</div>
<p>As mentioned earlier, clearly, <span class="math notranslate nohighlight">\(q_2\sim\mathcal{N}(1.00, 3.00)\)</span> seems closer to <span class="math notranslate nohighlight">\(p\)</span></p>
</section>
</section>
<section id="optimizing-the-kl-divergence-between-q-and-p">
<h2>Optimizing the KL-divergence between q and p<a class="headerlink" href="#optimizing-the-kl-divergence-between-q-and-p" title="Permalink to this headline">#</a></h2>
<p>We could create a grid of (loc, scale) pairs and find the best, as shown below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_values</span><span class="p">,</span> <span class="n">prob_values_p</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$p\sim\mathcal</span><span class="si">{N}</span><span class="s2">(1.00, 4.00)$&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>


<span class="k">for</span> <span class="n">loc</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]:</span>
    <span class="k">for</span> <span class="n">scale</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]:</span>
        <span class="n">q_d</span> <span class="o">=</span> <span class="n">q</span><span class="p">[(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">)]</span>
        <span class="n">kl_d</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">kl_divergence</span><span class="p">(</span><span class="n">q</span><span class="p">[(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">)],</span> <span class="n">p</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
            <span class="n">z_values</span><span class="p">,</span>
            <span class="n">q_d</span><span class="o">.</span><span class="n">prob</span><span class="p">(</span><span class="n">z_values</span><span class="p">),</span>
            <span class="n">label</span><span class="o">=</span><span class="sa">rf</span><span class="s2">&quot;$q\sim\mathcal</span><span class="se">{{</span><span class="s2">N</span><span class="se">}}</span><span class="s2">(</span><span class="si">{</span><span class="n">loc</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">scale</span><span class="si">}</span><span class="s2">)$&quot;</span>
            <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="o">+</span> <span class="sa">rf</span><span class="s2">&quot;$D_</span><span class="se">{{</span><span class="s2">KL</span><span class="se">}}</span><span class="s2">(q||p)$ = </span><span class="si">{</span><span class="n">kl_d</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.04</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">borderaxespad</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;PDF&quot;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/kl-divergence_15_0.png" src="../../_images/kl-divergence_15_0.png" />
</div>
</div>
<p>Or, we could use continuous optimization to find the best loc and scale parameters for q.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">to_train_q</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span>
    <span class="n">loc</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;loc&quot;</span><span class="p">),</span>
    <span class="n">scale</span><span class="o">=</span><span class="n">tfp</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">TransformedVariable</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">bijector</span><span class="o">=</span><span class="n">tfb</span><span class="o">.</span><span class="n">Exp</span><span class="p">(),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;scale&quot;</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">to_train_q</span><span class="o">.</span><span class="n">trainable_variables</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-02-04 14:55:19.564807: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&lt;tf.Variable &#39;loc:0&#39; shape=() dtype=float32, numpy=-1.0&gt;,
 &lt;tf.Variable &#39;scale:0&#39; shape=() dtype=float32, numpy=0.0&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">loss_and_grads</span><span class="p">(</span><span class="n">q_dist</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">kl_divergence</span><span class="p">(</span><span class="n">q_dist</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">q_dist</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">loss_and_grads</span><span class="p">(</span><span class="n">to_train_q</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">to_train_q</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">to_train_q</span><span class="o">.</span><span class="n">loc</span><span class="p">,</span> <span class="n">to_train_q</span><span class="o">.</span><span class="n">scale</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&lt;tf.Variable &#39;loc:0&#39; shape=() dtype=float32, numpy=0.98873746&gt;,
 &lt;TransformedVariable: name=scale, dtype=float32, shape=[], fn=&quot;exp&quot;, numpy=3.9999995&gt;)
</pre></div>
</div>
</div>
</div>
<p>After training, we are able to recover the scale and loc very close to that of <span class="math notranslate nohighlight">\(p\)</span></p>
</section>
<section id="animation">
<h2>Animation!<a class="headerlink" href="#animation" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">animation</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">tight_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>

<span class="n">to_train_q</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span>
    <span class="n">loc</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;loc&quot;</span><span class="p">),</span>
    <span class="n">scale</span><span class="o">=</span><span class="n">tfp</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">TransformedVariable</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">bijector</span><span class="o">=</span><span class="n">tfb</span><span class="o">.</span><span class="n">Exp</span><span class="p">(),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;scale&quot;</span><span class="p">),</span>
<span class="p">)</span>


<span class="k">def</span> <span class="nf">animate</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_values</span><span class="p">,</span> <span class="n">prob_values_p</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$p\sim\mathcal</span><span class="si">{N}</span><span class="s2">(1.00, 4.00)$&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">loss_and_grads</span><span class="p">(</span><span class="n">to_train_q</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">to_train_q</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>
    <span class="n">loc</span> <span class="o">=</span> <span class="n">to_train_q</span><span class="o">.</span><span class="n">loc</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">to_train_q</span><span class="o">.</span><span class="n">scale</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
        <span class="n">z_values</span><span class="p">,</span>
        <span class="n">to_train_q</span><span class="o">.</span><span class="n">prob</span><span class="p">(</span><span class="n">z_values</span><span class="p">),</span>
        <span class="n">label</span><span class="o">=</span><span class="sa">rf</span><span class="s2">&quot;$q\sim \mathcal</span><span class="se">{{</span><span class="s2">N</span><span class="se">}}</span><span class="s2">(</span><span class="si">{</span><span class="n">loc</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">scale</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">)$&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">d_kl</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">kl_divergence</span><span class="p">(</span><span class="n">to_train_q</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">rf</span><span class="s2">&quot;Iteration: </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">, $D_</span><span class="se">{{</span><span class="s2">KL</span><span class="se">}}</span><span class="s2">(q||p)$: </span><span class="si">{</span><span class="n">d_kl</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">borderaxespad</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">((</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">15</span><span class="p">))</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;PDF&quot;</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>


<span class="n">ani</span> <span class="o">=</span> <span class="n">animation</span><span class="o">.</span><span class="n">FuncAnimation</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">animate</span><span class="p">,</span> <span class="n">frames</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ani</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;kl_qp.gif&quot;</span><span class="p">,</span> <span class="n">writer</span><span class="o">=</span><span class="s2">&quot;imagemagick&quot;</span><span class="p">,</span> <span class="n">fps</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;Figure size 432x288 with 0 Axes&gt;
</pre></div>
</div>
</div>
</div>
<p><img alt="" src="notebooks/information_theory/kl_qp.gif" /></p>
</section>
<section id="finding-the-kl-divergence-for-two-distributions-from-different-families">
<h2>Finding the KL divergence for two distributions from different families<a class="headerlink" href="#finding-the-kl-divergence-for-two-distributions-from-different-families" title="Permalink to this headline">#</a></h2>
<p>Let us rework our example with <code class="docutils literal notranslate"><span class="pre">p</span></code> coming from  a mixture of Gaussian distribution and <code class="docutils literal notranslate"><span class="pre">q</span></code> being Normal.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">p_s</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">MixtureSameFamily</span><span class="p">(</span>
    <span class="n">mixture_distribution</span><span class="o">=</span><span class="n">tfd</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]),</span>
    <span class="n">components_distribution</span><span class="o">=</span><span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span>
        <span class="n">loc</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>  <span class="c1"># One for each component.</span>
    <span class="p">),</span>
<span class="p">)</span>  <span class="c1"># And same here.</span>

<span class="n">p_s</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tfp.distributions.MixtureSameFamily &#39;MixtureSameFamily&#39; batch_shape=[] event_shape=[] dtype=float32&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_values</span><span class="p">,</span> <span class="n">p_s</span><span class="o">.</span><span class="n">prob</span><span class="p">(</span><span class="n">z_values</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/kl-divergence_29_0.png" src="../../_images/kl-divergence_29_0.png" />
</div>
</div>
<p>Let us create two Normal distributions q_1 and q_2 and plot them to see which looks closer to p_s.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">q_1</span> <span class="o">=</span> <span class="n">create_q</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">q_2</span> <span class="o">=</span> <span class="n">create_q</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prob_values_p_s</span> <span class="o">=</span> <span class="n">p_s</span><span class="o">.</span><span class="n">prob</span><span class="p">(</span><span class="n">z_values</span><span class="p">)</span>
<span class="n">prob_values_q_1</span> <span class="o">=</span> <span class="n">q_1</span><span class="o">.</span><span class="n">prob</span><span class="p">(</span><span class="n">z_values</span><span class="p">)</span>
<span class="n">prob_values_q_2</span> <span class="o">=</span> <span class="n">q_2</span><span class="o">.</span><span class="n">prob</span><span class="p">(</span><span class="n">z_values</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_values</span><span class="p">,</span> <span class="n">prob_values_p_s</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;MOG&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_values</span><span class="p">,</span> <span class="n">prob_values_q_1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$q_1\sim\mathcal</span><span class="si">{N}</span><span class="s2"> (3, 1.0)$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_values</span><span class="p">,</span> <span class="n">prob_values_q_2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$q_2\sim\mathcal</span><span class="si">{N}</span><span class="s2"> (3, 4.5)$&quot;</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;PDF&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span>
    <span class="s2">&quot;dkl-different.png&quot;</span><span class="p">,</span>
    <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/kl-divergence_32_0.png" src="../../_images/kl-divergence_32_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="n">tfd</span><span class="o">.</span><span class="n">kl_divergence</span><span class="p">(</span><span class="n">q_1</span><span class="p">,</span> <span class="n">p_s</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>No KL(distribution_a || distribution_b) registered for distribution_a type Normal and distribution_b type MixtureSameFamily
</pre></div>
</div>
</div>
</div>
<p>As we see above, we can not compute the KL divergence directly. The core idea would now be to leverage the Monte Carlo sampling and generating the expectation. The following function does that.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">kl_via_sampling</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">100000</span><span class="p">):</span>
    <span class="c1"># Get samples from q</span>
    <span class="n">sample_set</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
    <span class="c1"># Use the definition of KL-divergence</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">sample_set</span><span class="p">)</span> <span class="o">-</span> <span class="n">p</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">sample_set</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">kl_via_sampling</span><span class="p">(</span><span class="n">q_1</span><span class="p">,</span> <span class="n">p_s</span><span class="p">),</span> <span class="n">kl_via_sampling</span><span class="p">(</span><span class="n">q_2</span><span class="p">,</span> <span class="n">p_s</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&lt;tf.Tensor: shape=(), dtype=float32, numpy=9.465648&gt;,
 &lt;tf.Tensor: shape=(), dtype=float32, numpy=46.48004&gt;)
</pre></div>
</div>
</div>
</div>
<p>As we can see from KL divergence calculations, <code class="docutils literal notranslate"><span class="pre">q_1</span></code> is closer to our Gaussian mixture distribution.</p>
</section>
<section id="optimizing-the-kl-divergence-for-two-distributions-from-different-families">
<h2>Optimizing the KL divergence for two distributions from different families<a class="headerlink" href="#optimizing-the-kl-divergence-for-two-distributions-from-different-families" title="Permalink to this headline">#</a></h2>
<p>We saw that we can calculate the KL divergence between two different distribution families via sampling. But, as we did earlier, will we be able to optimize the parameters of our target surrogate distribution? The answer is No! As we have introduced sampling. However, there is still a way – by reparameterization!</p>
<p>Our surrogate q in this case is parameterized by <code class="docutils literal notranslate"><span class="pre">loc</span></code> and <code class="docutils literal notranslate"><span class="pre">scale</span></code>. The key idea here is to generate samples from a standard normal distribution (loc=0, scale=1) and then apply an affine transformation on the generated samples to get the samples generated from q. See my other post on sampling from normal distribution to understand this better.</p>
<p>The loss can now be thought of as a function of <code class="docutils literal notranslate"><span class="pre">loc</span></code> and <code class="docutils literal notranslate"><span class="pre">scale</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_samples</span> <span class="o">=</span> <span class="mi">1000</span>


<span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">):</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">)</span>
    <span class="n">q_1</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="n">sample_set</span> <span class="o">=</span> <span class="n">q_1</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
    <span class="n">sample_set</span> <span class="o">=</span> <span class="n">loc</span> <span class="o">+</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">sample_set</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">sample_set</span><span class="p">)</span> <span class="o">-</span> <span class="n">p_s</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">sample_set</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Having defined the loss above, we can now optimize <code class="docutils literal notranslate"><span class="pre">loc</span></code> and <code class="docutils literal notranslate"><span class="pre">scale</span></code> to minimize the KL-divergence.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_iter</span> <span class="o">=</span> <span class="mi">150</span>
<span class="n">location_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n_iter</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">scale_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n_iter</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">loss_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n_iter</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>


<span class="n">loc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">scale</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">4.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iter</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">(</span><span class="n">persistent</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">tape</span><span class="o">.</span><span class="n">watch</span><span class="p">(</span><span class="n">loc</span><span class="p">)</span>
        <span class="n">tape</span><span class="o">.</span><span class="n">watch</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span>
        <span class="n">lo</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
    <span class="p">[</span><span class="n">dl_loc</span><span class="p">,</span> <span class="n">dl_scale</span><span class="p">]</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">lo</span><span class="p">,</span> <span class="p">[</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">50</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="n">lo</span><span class="p">,</span> <span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
    <span class="n">location_array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">loc</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">scale_array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">loss_array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">lo</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">([</span><span class="n">dl_loc</span><span class="p">,</span> <span class="n">dl_scale</span><span class="p">],</span> <span class="p">[</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>38.7589951 3 4
0.4969607 0.736858189 0.680736303
0.528585315 0.774057031 0.617758751
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">q_s</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">)</span>
<span class="n">q_s</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tfp.distributions.Normal &#39;Normal&#39; batch_shape=[] event_shape=[] dtype=float32&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prob_values_p_s</span> <span class="o">=</span> <span class="n">p_s</span><span class="o">.</span><span class="n">prob</span><span class="p">(</span><span class="n">z_values</span><span class="p">)</span>
<span class="n">prob_values_q_s</span> <span class="o">=</span> <span class="n">q_s</span><span class="o">.</span><span class="n">prob</span><span class="p">(</span><span class="n">z_values</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_values</span><span class="p">,</span> <span class="n">prob_values_p_s</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;p&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_values</span><span class="p">,</span> <span class="n">prob_values_q_s</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;q&quot;</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;PDF&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/kl-divergence_44_0.png" src="../../_images/kl-divergence_44_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prob_values_p_s</span> <span class="o">=</span> <span class="n">p_s</span><span class="o">.</span><span class="n">prob</span><span class="p">(</span><span class="n">z_values</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">tight_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">a</span><span class="p">(</span><span class="n">iteration</span><span class="p">):</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
    <span class="n">loc</span> <span class="o">=</span> <span class="n">location_array</span><span class="p">[</span><span class="n">iteration</span><span class="p">]</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">scale_array</span><span class="p">[</span><span class="n">iteration</span><span class="p">]</span>
    <span class="n">q_s</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">)</span>

    <span class="n">prob_values_q_s</span> <span class="o">=</span> <span class="n">q_s</span><span class="o">.</span><span class="n">prob</span><span class="p">(</span><span class="n">z_values</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_values</span><span class="p">,</span> <span class="n">prob_values_p_s</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;p&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_values</span><span class="p">,</span> <span class="n">prob_values_q_s</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;q&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Iteration </span><span class="si">{</span><span class="n">iteration</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">loss_array</span><span class="p">[</span><span class="n">iteration</span><span class="p">]</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="n">ani_mg</span> <span class="o">=</span> <span class="n">animation</span><span class="o">.</span><span class="n">FuncAnimation</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">frames</span><span class="o">=</span><span class="n">n_iter</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">location_array</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;loc&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">scale_array</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;scale&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Iterations&quot;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x1add94bb0&gt;
</pre></div>
</div>
<img alt="../../_images/kl-divergence_46_1.png" src="../../_images/kl-divergence_46_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ani_mg</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;kl_qp_mg.gif&quot;</span><span class="p">,</span> <span class="n">writer</span><span class="o">=</span><span class="s2">&quot;imagemagick&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><img alt="" src="notebooks/information_theory/kl_qp_mg.gif" /></p>
</section>
<section id="optimizing-the-kl-divergence-between-two-2d-distributions">
<h2>Optimizing the KL-divergence between two 2d distributions<a class="headerlink" href="#optimizing-the-kl-divergence-between-two-2d-distributions" title="Permalink to this headline">#</a></h2>
<p>Let us now repeat the same procedure but for two 2d Normal distributions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">p_2d</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">MultivariateNormalFullCovariance</span><span class="p">(</span>
    <span class="n">loc</span><span class="o">=</span><span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="n">covariance_matrix</span><span class="o">=</span><span class="p">[[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]</span>
<span class="p">)</span>

<span class="n">to_train_q_2d_2</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">MultivariateNormalDiag</span><span class="p">(</span>
    <span class="n">loc</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;loc&quot;</span><span class="p">),</span>
    <span class="n">scale_diag</span><span class="o">=</span><span class="n">tfp</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">TransformedVariable</span><span class="p">(</span>
        <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="n">bijector</span><span class="o">=</span><span class="n">tfb</span><span class="o">.</span><span class="n">Exp</span><span class="p">(),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;scale&quot;</span>
    <span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>WARNING:tensorflow:From /Users/nipun/miniforge3/lib/python3.9/site-packages/tensorflow_probability/python/distributions/distribution.py:342: MultivariateNormalFullCovariance.__init__ (from tensorflow_probability.python.distributions.mvn_full_covariance) is deprecated and will be removed after 2019-12-01.
Instructions for updating:
`MultivariateNormalFullCovariance` is deprecated, use `MultivariateNormalTriL(loc=loc, scale_tril=tf.linalg.cholesky(covariance_matrix))` instead.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>


<span class="k">def</span> <span class="nf">make_pdf_2d_gaussian</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">title</span><span class="p">):</span>
    <span class="n">N</span> <span class="o">=</span> <span class="mi">60</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>

    <span class="c1"># Pack X and Y into a single 3-dimensional array</span>
    <span class="n">pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span> <span class="o">+</span> <span class="p">(</span><span class="mi">2</span><span class="p">,))</span>
    <span class="n">pos</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span>
    <span class="n">pos</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">Y</span>

    <span class="n">F</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">MultivariateNormalFullCovariance</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">covariance_matrix</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">prob</span><span class="p">(</span><span class="n">pos</span><span class="p">)</span>

    <span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x_1$&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x_2$&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s2">&quot;equal&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">title</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;$\mu$ = </span><span class="si">{</span><span class="n">mu</span><span class="si">}</span><span class="se">\n</span><span class="s2"> $\Sigma$ = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">sigma</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;viridis&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;plasma&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">make_pdf_2d_gaussian</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="n">ax</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
<span class="n">make_pdf_2d_gaussian</span><span class="p">(</span>
    <span class="n">to_train_q_2d_2</span><span class="o">.</span><span class="n">loc</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">to_train_q_2d_2</span><span class="o">.</span><span class="n">covariance</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">ax</span><span class="p">,</span> <span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/kl-divergence_52_0.png" src="../../_images/kl-divergence_52_0.png" />
</div>
</div>
<p>As we can see above, the two distributions look very different. We can calculate the KL-divergence as before.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tfd</span><span class="o">.</span><span class="n">kl_divergence</span><span class="p">(</span><span class="n">to_train_q_2d_2</span><span class="p">,</span> <span class="n">p_2d</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tf.Tensor: shape=(), dtype=float32, numpy=4.8723755&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">tight_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">animate</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">kl_divergence</span><span class="p">(</span><span class="n">to_train_q_2d_2</span><span class="p">,</span> <span class="n">p_2d</span><span class="p">)</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">to_train_q_2d_2</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">to_train_q_2d_2</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>
    <span class="n">loc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">to_train_q_2d_2</span><span class="o">.</span><span class="n">loc</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">to_train_q_2d_2</span><span class="o">.</span><span class="n">covariance</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">make_pdf_2d_gaussian</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">make_pdf_2d_gaussian</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="n">ax</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x_1$&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x_2$&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s2">&quot;right&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s2">&quot;top&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># Only show ticks on the left and bottom spines</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_ticks_position</span><span class="p">(</span><span class="s2">&quot;left&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_ticks_position</span><span class="p">(</span><span class="s2">&quot;bottom&quot;</span><span class="p">)</span>


<span class="n">ani2</span> <span class="o">=</span> <span class="n">animation</span><span class="o">.</span><span class="n">FuncAnimation</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">animate</span><span class="p">,</span> <span class="n">frames</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ani2</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;kl_qp_2.gif&quot;</span><span class="p">,</span> <span class="n">writer</span><span class="o">=</span><span class="s2">&quot;imagemagick&quot;</span><span class="p">,</span> <span class="n">fps</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;Figure size 432x288 with 0 Axes&gt;
</pre></div>
</div>
</div>
</div>
<p><img alt="" src="notebooks/information_theory/kl_qp_2.gif" /></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">to_train_q_2d_2</span><span class="o">.</span><span class="n">loc</span><span class="p">,</span> <span class="n">to_train_q_2d_2</span><span class="o">.</span><span class="n">covariance</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&lt;tf.Variable &#39;loc:0&#39; shape=(2,) dtype=float32, numpy=array([ 0.01590762, -0.01590773], dtype=float32)&gt;,
 &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=
 array([[0.87550056, 0.        ],
        [0.        , 1.7570419 ]], dtype=float32)&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tfd</span><span class="o">.</span><span class="n">kl_divergence</span><span class="p">(</span><span class="n">to_train_q_2d_2</span><span class="p">,</span> <span class="n">p_2d</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tf.Tensor: shape=(), dtype=float32, numpy=0.0670591&gt;
</pre></div>
</div>
</div>
</div>
<p>We can now see that the KL-divergence has reduced significantly from where we started, but it will unlikely improve as ou
r <code class="docutils literal notranslate"><span class="pre">q</span></code> distribution is a multivariate diagonal normal distribution, meaning there is no correlation between the two dimensions.</p>
</section>
<section id="to-fix">
<h2>To-FIX<a class="headerlink" href="#to-fix" title="Permalink to this headline">#</a></h2>
<p>Everything below here needs to be fixed</p>
<section id="kl-divergence-and-elbo">
<h3>KL-Divergence and ELBO<a class="headerlink" href="#kl-divergence-and-elbo" title="Permalink to this headline">#</a></h3>
<p>Let us consider linear regression. We have parameters <span class="math notranslate nohighlight">\(\theta \in R^D\)</span> and we define a prior over them. Let us assume we define prior <span class="math notranslate nohighlight">\(p(\theta)\sim \mathcal{N_D} (\mu, \Sigma)\)</span>. Now, given our dataset <span class="math notranslate nohighlight">\(D = \{X, y\}\)</span> and a parameter vector <span class="math notranslate nohighlight">\(\theta\)</span>, we can deifine our likelihood as <span class="math notranslate nohighlight">\(p(D|\theta)\)</span> or <span class="math notranslate nohighlight">\(p(y|X, \theta) = \prod_{i=1}^{n} p(y_i|x_i, \theta) = \prod_{i=1}^{n} \mathcal{N}(y_i|x_i^T\theta, \sigma^2) \)</span></p>
<p>As per Bayes rule, we can obtain the posterior over <span class="math notranslate nohighlight">\(\theta\)</span> as:</p>
<p><span class="math notranslate nohighlight">\(p(\theta|D) = \dfrac{p(D|\theta)p(\theta)}{p(D)}\)</span></p>
<p>Now, in general <span class="math notranslate nohighlight">\(p(D)\)</span> is hard to compute.</p>
<p>So, in variational inference, our aim is to use a surrogate distribution <span class="math notranslate nohighlight">\(q(\theta)\)</span> such that it is very close to <span class="math notranslate nohighlight">\(p(\theta|D)\)</span>. We do so by minimizing the KL divergence between <span class="math notranslate nohighlight">\(q(\theta)\)</span> and <span class="math notranslate nohighlight">\(p(\theta|D)\)</span>.</p>
<p>Aim: $<span class="math notranslate nohighlight">\(q^*(\theta) = \underset{q(\theta) \in \mathcal{Q}}{\mathrm{argmin~}} D_{KL}[q(\theta)||p(\theta|D)]\)</span>$</p>
<p>Now,  $<span class="math notranslate nohighlight">\(D_{KL}[q(\theta)||p(\theta|D)] = \mathbb{E}_{q(\theta)}[\log\frac{q(\theta)}{p(\theta|D)}]\)</span><span class="math notranslate nohighlight">\(
Now,  \)</span><span class="math notranslate nohighlight">\( = \mathbb{E}_{q(\theta)}[\log\frac{q(\theta)p(D)}{p(\theta, D)}]\)</span><span class="math notranslate nohighlight">\(
Now,  \)</span><span class="math notranslate nohighlight">\( = \mathbb{E}_{q(\theta)}[\log q(\theta)]- \mathbb{E}_{q(\theta)}[\log p(\theta, D)] + \mathbb{E}_{q(\theta)}[\log p(D)] \)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(= \mathbb{E}_{q(\theta)}[\log q(\theta)]- \mathbb{E}_{q(\theta)}[\log p(\theta, D)] + \log p(D) \)</span>$</p>
<p>Now, <span class="math notranslate nohighlight">\(p(D) \in \{0, 1\}\)</span>. Thus, <span class="math notranslate nohighlight">\(\log p(D) \in \{-\infty, 0 \}\)</span></p>
<p>Now, let us look at the quantities:</p>
<div class="math notranslate nohighlight">
\[\underbrace{D_{KL}[q(\theta)||p(\theta|D)]}_{\geq 0} = \underbrace{\mathbb{E}_{q(\theta)}[\log q(\theta)]- \mathbb{E}_{q(\theta)}[\log p(\theta, D)]}_{-\text{ELBO(q)}} +  \underbrace{\log p(D)}_{\leq 0}\]</div>
<p>Thus, we know that <span class="math notranslate nohighlight">\(\log p(D) \geq \text{ELBO(q)}\)</span></p>
<p>Thus, finally we can rewrite the optimisation from</p>
<div class="math notranslate nohighlight">
\[q^*(\theta) = \underset{q(\theta) \in \mathcal{Q}}{\mathrm{argmin~}} D_{KL}[q(\theta)||p(\theta|D)]\]</div>
<p>to</p>
<div class="math notranslate nohighlight">
\[q^*(\theta) = \underset{q(\theta) \in \mathcal{Q}}{\mathrm{argmax~}} \text{ELBO(q)}\]</div>
<p>Now, given our linear regression problem setup, we want to maximize the ELBO.</p>
<p>We can do so by the following. As a simple example, let us assume <span class="math notranslate nohighlight">\(\theta \in R^2\)</span></p>
<ul class="simple">
<li><p>Assume some q. Say, a Normal distribution. So, <span class="math notranslate nohighlight">\(q\sim \mathcal{N}_2\)</span></p></li>
<li><p>Draw samples from q. Say N samples.</p></li>
<li><p>Initilize ELBO = 0.0</p></li>
<li><p>For each sample:</p>
<ul>
<li><p>Let us assume drawn sample is <span class="math notranslate nohighlight">\([\theta_1, \theta_2]^T\)</span></p></li>
<li><p>Compute log_prob of prior on <span class="math notranslate nohighlight">\([\theta_1, \theta_2]^T\)</span> or <code class="docutils literal notranslate"><span class="pre">lp</span> <span class="pre">=</span> <span class="pre">p.log_prob(θ1,</span> <span class="pre">θ2)</span></code></p></li>
<li><p>Compute log_prob of likelihood on <span class="math notranslate nohighlight">\([\theta_1, \theta_2]^T\)</span> or <code class="docutils literal notranslate"><span class="pre">ll</span> <span class="pre">=</span> <span class="pre">l.log_prob(θ1,</span> <span class="pre">θ2)</span></code></p></li>
<li><p>Compute log_prob of q on <span class="math notranslate nohighlight">\([\theta_1, \theta_2]^T\)</span> or <code class="docutils literal notranslate"><span class="pre">lq</span> <span class="pre">=</span> <span class="pre">q.log_prob(θ1,</span> <span class="pre">θ2)</span></code></p></li>
<li><p>ELBO = ELBO + (ll+lp-q)</p></li>
</ul>
</li>
<li><p>Return ELBO/N</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">lr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">stddv_datapoints</span><span class="p">):</span>
    <span class="n">num_datapoints</span><span class="p">,</span> <span class="n">data_dim</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">th</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span>
        <span class="n">loc</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">data_dim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">scale</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">data_dim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;theta&quot;</span>
    <span class="p">)</span>

    <span class="n">x_dash</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">x</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span>
        <span class="n">loc</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">matvec</span><span class="p">(</span><span class="n">x_dash</span><span class="p">,</span> <span class="n">th</span><span class="p">),</span> <span class="n">scale</span><span class="o">=</span><span class="n">stddv_datapoints</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">functools</span>

<span class="n">stddv_datapoints</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">concrete_lr_model</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">lr_2</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">stddv_datapoints</span><span class="o">=</span><span class="n">stddv_datapoints</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">JointDistributionCoroutineAutoBatched</span><span class="p">(</span><span class="n">concrete_lr_model</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">th_sample</span><span class="p">,</span> <span class="n">data_sample</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data_sample</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="o">*</span><span class="n">th_sample</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">th_sample</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">th_sample</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tf.Tensor([1.5110626  0.42292204], shape=(2,), dtype=float32)
</pre></div>
</div>
<img alt="../../_images/kl-divergence_66_1.png" src="../../_images/kl-divergence_66_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">th_sample</span><span class="p">,</span> <span class="n">data_sample</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tf.Tensor: shape=(), dtype=float32, numpy=-150.26591&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">scale</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>

<span class="n">q_to_learn</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;q_theta_learn&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_samples</span> <span class="o">=</span> <span class="mi">1000</span>


<span class="n">loc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">scale</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">TransformedVariable</span><span class="p">([</span><span class="mf">.7</span><span class="p">,</span> <span class="mf">.6</span><span class="p">],</span> <span class="n">bijector</span><span class="o">=</span><span class="n">tfb</span><span class="o">.</span><span class="n">SoftClip</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">loss</span><span class="p">():</span>
    <span class="n">q_to_learn</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;q_theta_learn&quot;</span><span class="p">)</span>
    <span class="n">q_1</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="p">[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
    <span class="n">sample_set</span> <span class="o">=</span> <span class="n">q_1</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
    <span class="n">log_joint</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">sample_set</span><span class="p">,</span> <span class="n">data_sample</span><span class="p">))</span>
    <span class="n">log_q</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">q_to_learn</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">sample_set</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">log_q</span> <span class="o">-</span> <span class="n">log_joint</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trace6000</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">6000</span><span class="p">,</span> 
                  <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="mf">0.0001</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loc</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tf.Variable &#39;Variable:0&#39; shape=(2,) dtype=float32, numpy=array([1.4298061 , 0.84917635], dtype=float32)&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scale</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;TransformedVariable: name=soft_clip, dtype=float32, shape=[2], fn=&quot;soft_clip&quot;, numpy=array([0.6512191, 0.5705266], dtype=float32)&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x1f021f4f0&gt;]
</pre></div>
</div>
<img alt="../../_images/kl-divergence_73_1.png" src="../../_images/kl-divergence_73_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loc</span><span class="p">,</span> <span class="n">th_sample</span><span class="p">,</span> <span class="n">scale</span>
<span class="c1">#scale =  tfp.util.TransformedVariable([1., 1.], bijector=tfb.Exp())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&lt;tf.Variable &#39;Variable:0&#39; shape=(2,) dtype=float32, numpy=array([2.0690155, 1.496477 ], dtype=float32)&gt;,
 &lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.5110626 , 0.42292204], dtype=float32)&gt;,
 &lt;TransformedVariable: name=soft_clip, dtype=float32, shape=[2], fn=&quot;soft_clip&quot;, numpy=array([0.5936036, 0.5403929], dtype=float32)&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data_sample</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="o">*</span><span class="n">loc</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">loc</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x1f044ec40&gt;]
</pre></div>
</div>
<img alt="../../_images/kl-divergence_75_1.png" src="../../_images/kl-divergence_75_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loss_mc</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;function __main__.loss_mc(loc, scale)&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">scale</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">TransformedVariable</span><span class="p">([</span><span class="mf">.7</span><span class="p">,</span> <span class="mf">.6</span><span class="p">],</span> <span class="n">bijector</span><span class="o">=</span><span class="n">tfb</span><span class="o">.</span><span class="n">SoftClip</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">loss_mc</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">):</span>
    <span class="n">q_to_learn</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;q_theta_learn&quot;</span><span class="p">)</span>
    <span class="n">q_1</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="p">[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
    <span class="n">sample_set</span> <span class="o">=</span> <span class="n">q_1</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
    <span class="n">log_joint</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">sample_set</span><span class="p">,</span> <span class="n">data_sample</span><span class="p">))</span>
    <span class="n">log_q</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">q_to_learn</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">sample_set</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">log_q</span> <span class="o">-</span> <span class="n">log_joint</span>

<span class="n">target_log_prob_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">th</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">log_prob</span><span class="p">((</span><span class="n">th</span><span class="p">,</span> <span class="n">data_sample</span><span class="p">))</span>




<span class="n">data_dim</span><span class="o">=</span><span class="mi">2</span>
<span class="n">qt_mean</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">([</span><span class="n">data_dim</span><span class="p">]))</span>
<span class="n">qt_stddv</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">TransformedVariable</span><span class="p">(</span>
    <span class="mf">1e-4</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">data_dim</span><span class="p">]),</span> <span class="n">bijector</span><span class="o">=</span><span class="n">tfb</span><span class="o">.</span><span class="n">Softplus</span><span class="p">()</span>
<span class="p">)</span>


<span class="k">def</span> <span class="nf">factored_normal_variational_model</span><span class="p">():</span>
    <span class="n">qt</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">qt_mean</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">qt_stddv</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;qt&quot;</span><span class="p">)</span>


<span class="n">surrogate_posterior</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">JointDistributionCoroutineAutoBatched</span><span class="p">(</span>
    <span class="n">factored_normal_variational_model</span>
<span class="p">)</span>

<span class="n">losses</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">vi</span><span class="o">.</span><span class="n">fit_surrogate_posterior</span><span class="p">(</span>
    <span class="n">target_log_prob_fn</span><span class="o">=</span><span class="n">target_log_prob_fn</span><span class="p">,</span>
    <span class="n">surrogate_posterior</span><span class="o">=</span><span class="n">surrogate_posterior</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.05</span><span class="p">),</span>
    <span class="n">num_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/nipun/miniforge3/lib/python3.9/site-packages/tensorflow_probability/python/internal/vectorization_util.py:87: UserWarning: Saw Tensor seed Tensor(&quot;seed:0&quot;, shape=(2,), dtype=int32), implying stateless sampling. Autovectorized functions that use stateless sampling may be quite slow because the current implementation falls back to an explicit loop. This will be fixed in the future. For now, you will likely see better performance from stateful sampling, which you can invoke by passing a Python `int` seed.
  warnings.warn(
/Users/nipun/miniforge3/lib/python3.9/site-packages/tensorflow_probability/python/internal/vectorization_util.py:87: UserWarning: Saw Tensor seed Tensor(&quot;seed:0&quot;, shape=(2,), dtype=int32), implying stateless sampling. Autovectorized functions that use stateless sampling may be quite slow because the current implementation falls back to an explicit loop. This will be fixed in the future. For now, you will likely see better performance from stateful sampling, which you can invoke by passing a Python `int` seed.
  warnings.warn(
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x1f0d9faf0&gt;]
</pre></div>
</div>
<img alt="../../_images/kl-divergence_78_1.png" src="../../_images/kl-divergence_78_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">qt_mean</span><span class="p">,</span> <span class="n">qt_stddv</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&lt;tf.Variable &#39;Variable:0&#39; shape=(2,) dtype=float32, numpy=array([1.5777218 , 0.46246716], dtype=float32)&gt;,
 &lt;TransformedVariable: name=softplus, dtype=float32, shape=[2], fn=&quot;softplus&quot;, numpy=array([0.01456336, 0.01367522], dtype=float32)&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data_sample</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="o">*</span><span class="n">qt_mean</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">qt_mean</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x1edd15ca0&gt;]
</pre></div>
</div>
<img alt="../../_images/kl-divergence_81_1.png" src="../../_images/kl-divergence_81_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">post_samples</span> <span class="o">=</span> <span class="n">surrogate_posterior</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>

<span class="n">post_samples</span><span class="o">.</span><span class="n">qt</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>WARNING:tensorflow:Note that RandomStandardNormal inside pfor op may not give same output as inside a sequential loop.
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tf.Tensor: shape=(5, 2), dtype=float32, numpy=
array([[1.5795265 , 0.500741  ],
       [1.5515635 , 0.46671686],
       [1.5585055 , 0.4617632 ],
       [1.5856469 , 0.44141397],
       [1.5763292 , 0.45420292]], dtype=float32)&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data_sample</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="o">*</span><span class="n">qt_mean</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">qt_mean</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x1f1059880&gt;]
</pre></div>
</div>
<img alt="../../_images/kl-divergence_83_1.png" src="../../_images/kl-divergence_83_1.png" />
</div>
</div>
<p>References</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=HUsznqt2V5I">https://www.youtube.com/watch?v=HUsznqt2V5I</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=x9StQ8RZ0ag&amp;list=PLISXH-iEM4JlFsAp7trKCWyxeO3M70QyJ&amp;index=9">https://www.youtube.com/watch?v=x9StQ8RZ0ag&amp;list=PLISXH-iEM4JlFsAp7trKCWyxeO3M70QyJ&amp;index=9</a></p></li>
<li><p><a class="reference external" href="https://colab.research.google.com/github/goodboychan/goodboychan.github.io/blob/main/_notebooks/2021-09-13-02-Minimizing-KL-Divergence.ipynb#scrollTo=gd_ev8ceII8q">https://colab.research.google.com/github/goodboychan/goodboychan.github.io/blob/main/_notebooks/2021-09-13-02-Minimizing-KL-Divergence.ipynb#scrollTo=gd_ev8ceII8q</a></p></li>
<li><p><a class="reference external" href="https://goodboychan.github.io/python/coursera/tensorflow_probability/icl/2021/09/13/02-Minimizing-KL-Divergence.html">https://goodboychan.github.io/python/coursera/tensorflow_probability/icl/2021/09/13/02-Minimizing-KL-Divergence.html</a></p></li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "code-first-ml/book1",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks/information_theory"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="../mixture_models/2022-02-14-GMM.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">GMM learning</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="jensen-inequality.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">&lt;no title&gt;</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Jupyter Book community<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>
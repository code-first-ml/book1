{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out of Tensor factorisation\n",
    "> Out of tensor factorisation\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Nipun Batra\n",
    "- categories: [ML]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a [previous post](./nmf-out-matrix.html), we had looked at predicting for users who weren't a part of the original matrix factorisation. In this post, we'll look at the same for 3-d tensors. In case you want to learn more about tensor factorisations, look at my earlier [post](./tensor-decomposition-autograd.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Tensor Factorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://nipunbatra.github.io/blog/images/tf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General tensor factorisation for a 3d tensor A (M X N X O) would produce 3 factors- X (M X K), Y (N X K) and Z (O X K). The $A_{ijl}$ entry can be found as (Khatri product) :\n",
    "\n",
    "$$ A_{ijl} = \\sum_k{X_{ik}Y_{jk}Z_{lk}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning $X_M$ factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we'd assume that the $M^{th}$ entry isn't a part of this decomposition. So, how do we obtain the X factors correspondonding to $M^{th}$ entry? We learn the Y and Z factors from the tensor A (excluding the $M^{th}$ row entries). We assume the Y and Z learnt to be shared across the entries across rows of A (1 through M). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://nipunbatra.github.io/blog/images/tf-last-row.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure shows the **latent factor for X ($X_{M}$) corresponding to the $M^{th}$ entry of X that we wish to learn**. On the LHS, we see the matrix corresponding to $A_{M}$. The highlighted entry of $A_{M}$ is created by element-wise multiplication of $X_M, Y_0, Z_0$ and then summing. Thus, each of the N X O entries of $A_M$ are created by multiplying $X_M$ with a row from Y and a row from Z. In general, \n",
    "\n",
    "$$A_{M, n, o}  = \\sum_k{X_{M, k} \\times Y_{n, k} \\times Z_{o, k}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to learn $X_M$, we plan to use least squares. For that, we need to reduce the problem into $\\alpha x = \\beta$ We do this as follows:\n",
    "\n",
    "1. We flatten out the A_M matrix into a vector containing N X O entries and call it $\\beta$\n",
    "2. We create a matrix by element-wise multiplication of each row of Y with each row of Z to create $\\alpha$ of shape (N X O, K)\n",
    "\n",
    "We can now write, \n",
    "\n",
    "$$ \\alpha X_M^T \\approx \\beta $$\n",
    "Thus, X_M^T = Least Squares ($\\alpha, \\beta$)\n",
    "\n",
    "Ofcourse, $\\beta$ can have missing entries, which we mask out. Thus, we can write:\n",
    "\n",
    "$X_M^T$ = Least Squares ($\\alpha [Mask], \\beta [Mask]$)\n",
    "\n",
    "In case we're doing a non-negative tensor factorisation, we can instead learn $X_M^T$ as follows:\n",
    "$X_M^T$ = Non-negative Least Squares ($\\alpha [Mask], \\beta [Mask]$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorly\n",
    "from tensorly.decomposition import parafac, non_negative_parafac\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the tensor to be decomposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   1.,   2.],\n",
       "       [  3.,   4.,   5.],\n",
       "       [  6.,   7.,   8.],\n",
       "       [  9.,  10.,  11.]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M, N, O = 10, 4, 3 #user, movie, feature\n",
    "t = np.arange(M*N*O).reshape(M, N, O).astype('float32')\n",
    "t[0] #First entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting a few entries of the last user to be unavailable/unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  nan,  109.,  110.],\n",
       "       [ 111.,  112.,  113.],\n",
       "       [ 114.,  115.,   nan],\n",
       "       [ 117.,  118.,  119.]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_orig = t.copy() # creating a copy\n",
    "t[-1,:,:][0, 0] = np.NAN\n",
    "t[-1,:,:][2, 2] = np.NAN\n",
    "t[-1,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Non-negative PARAFAC decomposition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2\n",
    "# Notice, we factorise a tensor with one less user. thus, t[:-1, :, :]\n",
    "X, Y, Z = non_negative_parafac(t[:-1,:,:], rank=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9, 2), (4, 2), (3, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape, Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.48012616,  1.13542261],\n",
       "       [ 0.49409014,  2.98947262],\n",
       "       [ 0.5072998 ,  5.03375154],\n",
       "       [ 0.52051081,  7.07682331]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.57589198,  1.55655956],\n",
       "       [ 0.58183329,  1.7695134 ],\n",
       "       [ 0.58778163,  1.98182137]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating $\\alpha$ by element-wise multiplication of Y, Z and reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.27650081   1.76735291]\n",
      " [  0.27935339   2.00914552]\n",
      " [  0.28220934   2.25020479]\n",
      " [  0.28454255   4.65329218]\n",
      " [  0.28747809   5.28991186]\n",
      " [  0.29041711   5.92460074]\n",
      " [  0.29214989   7.83533407]\n",
      " [  0.29516391   8.9072908 ]\n",
      " [  0.29818151   9.9759964 ]\n",
      " [  0.299758    11.01549695]\n",
      " [  0.30285052  12.52253365]\n",
      " [  0.30594669  14.02499969]]\n",
      "\n",
      "Shape of alpha =  (12, 2)\n"
     ]
    }
   ],
   "source": [
    "alpha = np.einsum('nk, ok -> nok', Y, Z).reshape((N*O, K))\n",
    "print alpha\n",
    "\n",
    "print \"\\nShape of alpha = \", alpha.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import nnls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating $\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 109.],\n",
       "       [ 110.],\n",
       "       [ 111.],\n",
       "       [ 112.],\n",
       "       [ 113.],\n",
       "       [ 114.],\n",
       "       [ 115.],\n",
       "       [ 117.],\n",
       "       [ 118.],\n",
       "       [ 119.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = t[-1,:,:].reshape(N*O, 1)\n",
    "mask = ~np.isnan(beta).flatten()\n",
    "beta[mask].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning $X_M$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 389.73825036,    0.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_M = nnls(alpha[mask], beta[mask].reshape(-1, ))[0].reshape((1, K))\n",
    "X_M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing X_M with other entries from X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.40340055e-01,   7.62705972e-01],\n",
       "       [  4.14288653e+01,   7.57249713e-01],\n",
       "       [  8.51282259e+01,   6.56239315e-01],\n",
       "       [  1.29063811e+02,   5.46019997e-01],\n",
       "       [  1.73739412e+02,   4.06496594e-01],\n",
       "       [  2.19798887e+02,   2.11453297e-01],\n",
       "       [  2.64609697e+02,   6.54705290e-02],\n",
       "       [  3.01392149e+02,   2.39700484e-01],\n",
       "       [  3.39963876e+02,   3.41824756e-01]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the first column captures the increasing trend of values in the tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting missing entries using tensor multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 108.,  109.,  110.],\n",
       "        [ 111.,  112.,  113.],\n",
       "        [ 114.,  115.,  116.],\n",
       "        [ 117.,  118.,  119.]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.einsum('ir, jr, kr -> ijk', X_M, Y, Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 108.,  109.,  110.],\n",
       "       [ 111.,  112.,  113.],\n",
       "       [ 114.,  115.,  116.],\n",
       "       [ 117.,  118.,  119.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_orig[-1, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! We're exactly there!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

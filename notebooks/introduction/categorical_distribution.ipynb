{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53cded78",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Categorical Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f79695c",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "As we know, Bernoulli distribution allowed us to model discrete random variable with only two states, \n",
    "what if we wanted to model more than two or may be even hundreds. This is where Categorical Distribution comes into play, it’s a generalization of the Bernoulli distribution for a categorical random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c13bea",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " There are some general criteria for a distribution to be a categorical distribution:\n",
    "   - The categories are discrete.\n",
    "   - There are two or more potential categories.\n",
    "   - The sum of the probabilities for all categories must sum to 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b62e9bb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Lets take a finite set of label, $ x\\ \\in\\ \\{1,...,C\\} $\n",
    "\n",
    "PMF:\n",
    "\\begin{equation}\n",
    "    p(X = x| \\vec{\\theta}) = \\prod_{c=1}^{C} \\theta_c ^{I(x=c)}\\ ,\\ where\\ I(x=c) = \\begin{cases}\n",
    "                                                                              1, & \\text{if}\\ x==c \\\\\n",
    "                                                                              0, & \\text{otherwise}\n",
    "                                                                            \\end{cases} \\tag{eq. 1}\n",
    "\\end{equation}\n",
    "In other words,  $p(X = c|\\vec{\\theta}) = θ_c$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0d4c79",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Lets take the famous example of rolling a dice:\n",
    "\n",
    "There are K = 6 categorical outcomes and the probability for each outcome is 1/6.\n",
    "\n",
    "Sample space would be : { 0, 1, 2, 3, 4, 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb4119de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import distrax\n",
    "import optax\n",
    "\n",
    "key = jax.random.PRNGKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d42c295",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "theta = [1/6]*6\n",
    "\n",
    "cat = distrax.Categorical(probs=theta)\n",
    "n = 100 # no_of_samples\n",
    "cat_samples = cat.sample(seed = key, sample_shape=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22ea76e0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 different category in samples: \n",
      "[0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "category = jnp.unique(cat_samples)\n",
    "print(\"There are {} different category in samples: \".format(len(category)))\n",
    "print(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0461f5e2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 18, 4: 21, 0: 17, 5: 15, 1: 16, 3: 13})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see how many no. of occurence of each category are in samples.\n",
    "from collections import Counter\n",
    "\n",
    "count_cat_samples = Counter(cat_samples.tolist())\n",
    "count_cat_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64c83662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16666667 0.16666667 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "# we can find out probs by inbuilt functions\n",
    "print(cat.prob([1, 0, 6, 7, -1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72097542",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PMF AND CDF PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ed109d9",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13505266",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f94d4954c4d41558fb89740e5fabf06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.1, description='a', max=1.0, min=0.01, step=0.05), FloatSlider(value…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.draw_plots(a=0.1, b=0.1, c=0.1)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def draw_plots(a=0.1,b=0.1,c=0.1):\n",
    "    d = 1 - jnp.sum(jnp.array([a,b,c]))\n",
    "    theta = [a,b,c,d]\n",
    "    \n",
    "    cat = distrax.Categorical(probs=jnp.array(theta))\n",
    "    n = 100 # no_of_samples\n",
    "    cat_samples = cat.sample(seed = key, sample_shape=n)\n",
    "    \n",
    "    category = jnp.unique(cat_samples)\n",
    "    pdf = cat.prob(cat_samples)\n",
    "    cdf = cat.cdf(cat_samples)\n",
    "\n",
    "    fig, ax = plt.subplots(1,2,figsize=(12,5))\n",
    "\n",
    "    ax[0].stem(cat_samples, pdf)\n",
    "    ax[0].set_xlabel(\"x\", fontsize=15)\n",
    "    ax[0].set_ylabel(\"P(x)\", fontsize=15)\n",
    "    ax[0].set_ylim(0,1)\n",
    "    ax[0].spines['top'].set_color('none')\n",
    "    ax[0].spines['right'].set_color('none')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ax[1].bar(cat_samples,cdf)\n",
    "    \n",
    "    ax[1].set_xlabel(\"x\", fontsize=15)\n",
    "    ax[1].set_ylabel(\"CDF(x)\", fontsize=15)  \n",
    "    ax[1].spines['top'].set_color('none')\n",
    "    ax[1].spines['right'].set_color('none')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "interact(draw_plots,a=(0.01,1,0.05),b=(0.01,1,0.05),c=(0.01,1,0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fa8cb9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Maximum Likelihood Estimation of Categorical distribution\n",
    "\n",
    "The likelihood that the sample X follows the distribution defined by the set of parameters $\\theta $ \n",
    "equals the product of the likelihoods of the individual instances $x_t$.<br>\n",
    "Let us say we have observed some data $D = [x_1,x_2...x_n ]$,\n",
    "\n",
    "\\begin{equation}\n",
    "L(\\vec{\\theta}|D) ≡ p(D|\\vec{\\theta}) = \\prod_{t=1}^{N} p(x_t|\\vec{\\theta}) \\tag{eq. 2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818c08cd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our goal is to find the set of parameters θ that maximizes the likelihood estimation $L(θ|D)$.\n",
    "This is given by :\n",
    "\\begin{equation}\n",
    "\\vec{\\theta}^∗ =  argmax_\\theta (L(\\vec{\\theta}| D))\n",
    "\\end{equation}\n",
    " So,\n",
    "\\begin{align} \n",
    "L(\\vec{\\theta}|D) &=  \\prod_{t=1}^{N} \\prod_{c=1}^{C} \\theta_c ^{I(x_t\\ =\\ c)} \\tag{from eq. 1 and eq. 2} \\\\ \\\\\n",
    "    l &= \\log{L(\\vec{\\theta}|D)} \\tag{eq. 3} \\\\ \\\\\n",
    "\\end{align}\n",
    "Maximizing the likelihood function derived above can be a complex operation, so we can simplify it by taking the natural logarithm of the equation 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0170fec",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<br>\n",
    "\\begin{equation}\n",
    "l = \\sum_{t=1}^{N} \\sum_{c=1}^{C} I(x_t = c) \\log{(\\theta_c)} \\tag{eq. 4}\n",
    "\\end{equation}\n",
    "The logarithm of a function is also a monotonically increasing function. So maximizing the logarithm of the likelihood function, would also be equivalent to maximizing the likelihood function.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6b1ca0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Programmatically Solve MLE of Categorical Distribution\n",
    "\n",
    "So, we will use a gradient descent algorithm for minimization of the negative likelihood function, which would also be equivalent to maximizing the likelihood function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76b5065",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Before we use gradient descent algorithm, we need following primary parameters,\n",
    "1. Loss Function : ( we use negative of equation 4.)\n",
    "2. Initial values of $\\vec{\\theta}$\n",
    "3. Learning rate : which scales the gradient and controls the step size of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "993f2c66",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Negative of likelihood function of categorical distribution (eq. 4.)\n",
    "def loss_function(thetas, samples):\n",
    "    thetas = jax.nn.softmax(thetas)\n",
    "    result_fit = distrax.Categorical(probs= thetas)\n",
    "    return -jnp.sum(result_fit.log_prob(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3d0bc1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize default parametrs\n",
    "learning_rate = 1e-4\n",
    "init_thetas = jnp.array([0.5,0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3f8e8578",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, samples, thetas):\n",
    "        self.samples = samples\n",
    "        self.thetas = thetas \n",
    "        self.grads = [0]*len(thetas)\n",
    "        \n",
    "    def probs(self):\n",
    "        return jax.nn.softmax(self.thetas)\n",
    "    \n",
    "    def update(self):       \n",
    "        for i in range(len(self.thetas)):\n",
    "            new_theta = self.thetas[i] - self.rate * self.grads[i]\n",
    "            self.thetas = self.thetas.at[i].set(new_theta)\n",
    "\n",
    "    def fit(self, optimizer, epochs):\n",
    "        opt_state = optimizer.init(self.thetas)\n",
    "        for i in range(epochs):    \n",
    "            loss, self.grads = jax.value_and_grad(loss_function)(self.thetas, self.samples)\n",
    "            \n",
    "            updates, opt_state = optimizer.update(self.grads, opt_state, self.thetas)\n",
    "            self.thetas = optax.apply_updates(self.thetas, updates)\n",
    "            \n",
    "            if i%10 == 0:\n",
    "                print(f\"Loss at epoch {i} : \",loss)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d1809c3e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Lets generate some samples \n",
    "cat = distrax.Categorical(probs = [0.2, 0.8])\n",
    "samples = cat.sample(seed = jax.random.PRNGKey(1), sample_shape = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e8bc1c44",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 0 :  693.1472\n",
      "Loss at epoch 10 :  472.3168\n",
      "Loss at epoch 20 :  486.04886\n",
      "Loss at epoch 30 :  469.9006\n",
      "Loss at epoch 40 :  471.88266\n",
      "Loss at epoch 50 :  470.1206\n",
      "Loss at epoch 60 :  469.98138\n",
      "Loss at epoch 70 :  469.9761\n",
      "Loss at epoch 80 :  469.87805\n",
      "Loss at epoch 90 :  469.8786\n"
     ]
    }
   ],
   "source": [
    "optimizer = optax.adam(learning_rate = 0.1)\n",
    "\n",
    "model = Model(samples, init_thetas)\n",
    "model.fit(optimizer=optimizer, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c2b3f82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial :  [0.5 0.5]\n",
      "Learned :  [0.18020473 0.81979525]\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial : \", init_thetas)\n",
    "print(\"Learned : \", model.probs())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f70d336",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Theoretical Derivation of MLE for Categorical distribution\n",
    "Before we can differentiate the log-likelihood to find the maximum,<br>\n",
    "we need to introduce the constraint that all probabilities $\\theta $ sum up to 1, that is :\n",
    "\\begin{equation}\n",
    "\\sum_{c=1}^{C} \\theta_c = 1 \\tag{eq. 5}\n",
    "\\end{equation}\n",
    "\n",
    "Then we use the lagrangian function with the constraint than has the following form :\n",
    "\\begin{equation}\n",
    "l(\\theta, \\lambda) = \\log{(L(\\theta))} + \\lambda(1 - \\sum_{c=1}^{C} \\theta_c) \\tag{eq. 6}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b717f2c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To find the maximum, we differentiate the lagrangian w.r.t. $\\theta$ as follows:\n",
    "\\begin{align}\n",
    "\\frac{\\partial l}{\\partial \\theta_c} &= \\frac{\\sum_{t=1}^{N}I(x_t = c)}{\\theta_c}  - \\lambda = 0  \\tag{from eq. 4},   \\\\\n",
    "                             \\lambda &= \\frac{\\sum_{t=1}^{N}I(x_t = c)}{\\theta_c}, \\\\\n",
    "\\therefore                  \\theta_c &= \\frac{N_c}{\\lambda} \\tag{$N_c$ = no. of occurences that belongs to $\\it{c}$ category}\\\\\n",
    "                                     &\\tag{eq. 7}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06a7a01",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To solve for $\\lambda$, we sum both side and make use of our initial constraint:\n",
    "\\begin{align}\n",
    "               \\theta_c &= \\frac{N_c}{\\lambda}, \\\\\n",
    "   \\sum_{c=1}^{C}\\theta &= \\frac{1}{\\lambda} \\sum_{c=1}^{C}N_c, \\\\\n",
    "                    1   &= \\frac{1}{\\lambda} n \\tag{n = sum of all $N_c$} \\\\\n",
    "     \\therefore \\lambda &= n\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127eb699",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let put $\\lambda$ in eq 7. that giving us the MLE for $\\theta$:\n",
    "\\begin{align}\n",
    "        \\theta_c &= \\frac{N_c}{n}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f04dac8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta's values \t\t: [0.3, 0.3, 0.4]\n",
      "Theta's values by MLE\t: [0.3, 0.3, 0.4]\n"
     ]
    }
   ],
   "source": [
    "# Lets find out MlE for above distribution\n",
    "key = jax.random.PRNGKey(1)\n",
    "\n",
    "n2 = 10000\n",
    "true_thetas = [0.3,0.3,0.4]\n",
    "\n",
    "cat2 = distrax.Categorical(probs=true_thetas)\n",
    "cat_samples2 = cat2.sample(seed = key, sample_shape=n2)\n",
    "category = jnp.unique(cat_samples2)\n",
    "\n",
    "mle_thetas = [round(float(jnp.sum(cat_samples2 == i)/n2),2) for i in category]\n",
    "\n",
    "\n",
    "print(\"Theta's values \\t\\t:\",true_thetas)\n",
    "print(\"Theta's values by MLE\\t:\",mle_thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b47132a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

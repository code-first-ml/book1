
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Variational Inference &#8212; Prog-ML</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet">
  <link href="../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script kind="utterances">

    var commentsRunWhenDOMLoaded = cb => {
    if (document.readyState != 'loading') {
        cb()
    } else if (document.addEventListener) {
        document.addEventListener('DOMContentLoaded', cb)
    } else {
        document.attachEvent('onreadystatechange', function() {
        if (document.readyState == 'complete') cb()
        })
    }
}

var addUtterances = () => {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://utteranc.es/client.js";
    script.async = "async";

    script.setAttribute("repo", "prog-ml/prog-ml.github.io");
    script.setAttribute("issue-term", "pathname");
    script.setAttribute("theme", "github-light");
    script.setAttribute("label", "üí¨ comment");
    script.setAttribute("crossorigin", "anonymous");

    sections = document.querySelectorAll("div.section");
    if (sections !== null) {
        section = sections[sections.length-1];
        section.appendChild(script);
    }
}
commentsRunWhenDOMLoaded(addUtterances);
</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Bayesian linear regression" href="../bayesian_ml_with_pymc/2021-03-11-blr-pymc.html" />
    <link rel="prev" title="&lt;no title&gt;" href="../information_theory/jensen-inequality.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/prog-ml.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Prog-ML</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../intro.html">
   Prog-ML
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/mle_coin.html">
   MLE, MAP and Fully Bayesian (conjugate prior and MCMC) for coin toss
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/variational.html">
   Variational Inference from scratch in JAX
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Probability Basics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../probability/sample-space.html">
   Sample Space
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../probability/random-variable.html">
   Random Variables
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../probability/pmf.html">
   Probability Mass Function (PMF)
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Continuous Probability Distributions
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../probability/univariate-normal.html">
   Properties of RV
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../probability/univariate-normal-expectations.html">
   Derivations for moments of univariate normal distribution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../probability/mvn-introduction.html">
   Multivariate Normal Distribution: Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../probability/mvn-marginal.html">
   Multivariate Normal Distribution: Marginals
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Bayesian ML
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml/2021-03-23-bayesian-ml.html">
   Bayesian ML: Fundamentals
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml/2021-04-14-bayesian-linear-regression.html">
   Bayesian linear regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml/2021-03-29-bayesian-model-selection.html">
   Bayesian model selection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml/2021-03-27-Marginal-Likelihood-2.html">
   Marginal likelihood
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml/2021-03-31-derivation-of-marginal-likelihood.html">
   Marginal likelihood for Bayesian linear regression
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Sampling from Distributions
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../sampling/rejection-sampling-lr.html">
   Simple rejection sampling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../sampling/Metropolis-Hastings.html">
   Metropolis Hastings
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../sampling/2021-03-10-importance-sampling.html">
   Importance sampling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../sampling/2021-03-10-rejection-sampling.html">
   Rejection sampling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../sampling/2022-02-04-sampling-normal.html">
   Sampling from univariate and multivariate normal distributions using Box-Muller transform
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../sampling/2020-04-16-inverse-transform.html">
   Sampling from common distributions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../sampling/2014-05-01-gibbs-sampling.html">
   Gibbs sampling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../sampling/2014-07-01-mcmc_coins.html">
   Coin tosses and MCMC
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Graphical Models
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../graphical_models/2022-02-15-draw-graphical-models.html">
   Drawing graphical models
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Mixture Models
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../mixture_models/2022-02-14-GMM.html">
   GMM learning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Information Theory
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../information_theory/kl-divergence.html">
   Understanding KL-Divergence
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Variational Models
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Variational Inference
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Bayesian ML with PyMC
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml_with_pymc/2021-03-11-blr-pymc.html">
   Bayesian linear regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml_with_pymc/2021-03-12-logistic-bayesian.html">
   Bayesian logistic regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../gaussian_processes/2021-03-16-GP-PyMC3.html">
   Gaussian process regression in PyMC
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../gaussian_processes/2021-03-17-lls-gp-pymc3.html">
   Local Lengthscale GP with PyMC
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Bayesian ML with Pyro
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml_with_pyro/2021-08-20-Bayesian.html">
   Probabilistic Programming in Pyro
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml_with_pyro/2022_02_17_pyro_linreg.html">
   Linear Regression using Pyro
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml_with_pyro/2022-02-20-condition-pyro.html">
   Pyro Conditioning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Bayesian ML with PyTorch
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml_with_pytorch/2022-02-09-pytorch-learn-normal.html">
   Maximum Likelihood Estimation (MLE) for parameters of univariate and multivariate normal distribution in PyTorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml_with_pytorch/2022-02-11-pytorch-learn-normal-map.html">
   Maximum A-Posteriori (MAP) for parameters of univariate and multivariate normal distribution in PyTorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml_with_pytorch/2022-02-17-ppca.html">
   Probabilstic PCA using PyTorch distributions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml_with_pytorch/2022-02-14-logistic-regression.html">
   Logistic Regression using PyTorch distributions
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Bayesian ML with Tensorflow Probability
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml_with_tfp/2022-01-26-tfp-distributions.html">
   Testing out some distributions in Tensorflow Probability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml_with_tfp/2022-02-07-coin-toss.html">
   Coin Toss (MLE, MAP, Fully Bayesian) in TF Probability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml_with_tfp/2022-01-28-tfp-linear-regression.html">
   Linear Regression in Tensorflow Probability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml_with_tfp/2022-02-05-lr.html">
   Linear Regression in TF Probability using JointDistributionCoroutineAutoBatched
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml_with_tfp/2022-02-05-simple-dgm.html">
   Simple Directed Graphical Models in TF Probability
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Bayesian ML with Julia
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian_ml_with_julia/2021-09-01-Hello-Julia-Language.html">
   Linear Regression from scratch in Julia
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Gaussian Processes
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../gaussian_processes/2020-03-26-gp.html">
   Some experiments in Gaussian Processes Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../gaussian_processes/2021-04-15-LLS-GP.html">
   Local Lengthscale GP
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../gaussian_processes/2021-04-15-deep-gp-from-scratch.html">
   Deep Kernel Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../gaussian_processes/2021-04-16-ard-gp.html">
   Automatic relevance determination (ARD)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../gaussian_processes/2021-04-16-GP-vs-DeepGP.html">
   GP v/s Deep GP on 2d data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../gaussian_processes/2022-02-23-gp_rff.html">
   Gaussian Processes with Random Fourier Features
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../gaussian_processes/2020-03-29-param-learning.html">
   Learning Gaussian Process regression parameters using gradient descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../gaussian_processes/2021-09-03-param-learning-sgd.html">
   Learning Gaussian Process regression parameters using mini-batch stochastic gradient descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../gaussian_processes/2020-06-26-gp-understand.html">
   Understanding Kernels in Gaussian Processes Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../gaussian_processes/pyro-deep-gp.html">
   Deep Kernel Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../gaussian_processes/pyro-binary-classification.html">
   GP Classification
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Linear Regression
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../linear_regression/2022_02_21_coordinate_descent_failure.html">
   Coordinate descent failure example
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Generative Adversarial Networks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../gans/2021-05-31-GAN.html">
   A programming introduction to GANs
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Hidden Markov Models
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../hmms/2013-06-01-hmm_simulate.html">
   HMM Simulation for Unfair Casino Problem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../hmms/2013-07-01-hmm_continuous.html">
   HMM Simulation for Continuous HMM
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tensor Factorization
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../tensor_factorization/2017-04-19-nmf-out-matrix.html">
   Out of matrix non-negative matrix factorisation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tensor_factorization/2017-04-20-parafac-out-tensor.html">
   Out of Tensor factorisation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tensor_factorization/2017-08-13-mf-autograd-adagrad.html">
   Adagrad based matrix factorization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tensor_factorization/2017-04-21-constrained-nmf-cvx.html">
   Constrained Non-negative matrix factorisation using CVXPY
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Neural Networks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../neural_networks/2020-03-08-keras-neural-non-linear.html">
   Some Neural Network Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neural_networks/2020-02-28-xor-relu-vector.html">
   Learning neural network for XOR
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neural_networks/2020-03-02-linear-scratch.html">
   Neural Networks from scratch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neural_networks/2018-01-13-denoising.html">
   Signal denoising using RNNs in PyTorch
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Expectation Maximization
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../em/2014-06-01-em.html">
   Programatically understanding Expectation Maximization
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Recommender Systems
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../recommender_systems/2017-12-18-recommend-keras.html">
   Recommender Systems in Keras
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../recommender_systems/2017-12-29-neural-collaborative-filtering.html">
   Neural Networks for Collaborative Filtering
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Active Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../active_learning/2018-06-20-active-committee.html">
   Active Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../active_learning/2022_01_24_Query_by_Committee.html">
   Query by Committee
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../active_learning/2020-04-21-active-learning-with-bayesian-linear-regression.html">
   Active Learning with Bayesian Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../active_learning/2022-03-06-maximal-expected-error-reduction.html">
   Problem with Expected Model Change
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  JAX
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../jax/introduction-jax.html">
   Using PRNG key
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ml_softwares/2022-02-09-autograd-pytorch-jax.html">
   Autograd in JAX and PyTorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ml_softwares/2017-08-12-linear-regression-adagrad-vs-gd.html">
   Programatically understanding Adagrad
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Appendix 1 - Linear Algebra for ML
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../appendix_1_-_linear_algebra_for_ml/2021-03-15-eigen.html">
   Eigen values
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../appendix_1_-_linear_algebra_for_ml/2021-03-15-determinant.html">
   Determinant
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../appendix_1_-_linear_algebra_for_ml/2021-03-15-Positive-semi-definite.html">
   Positive definiteness
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../appendix_1_-_linear_algebra_for_ml/2022-02-11-matrix.html">
   Matrix as transformation and interpreting low rank matrix
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Appendix 2 - Stochastic processes
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../appendix_2_-_stochastic_processes/2021-03-19-stochastic-processes.html">
   Stochastic processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../appendix_2_-_stochastic_processes/2021-03-17-Stationary-Time_Series.html">
   Stationarity of time-series stochastic process
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../appendix_2_-_stochastic_processes/2021-03-23-Stationarity-stochastic-processes.html">
   Stationarity of stochastic processes II
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../references/references.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/notebooks/variational_models/2022-02-12-variational-inference.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/prog-ml/prog-ml.github.io"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/prog-ml/prog-ml.github.io/issues/new?title=Issue%20on%20page%20%2Fnotebooks/variational_models/2022-02-12-variational-inference.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/prog-ml/prog-ml.github.io/edit/main/notebooks/variational_models/2022-02-12-variational-inference.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/prog-ml/prog-ml.github.io/main?urlpath=tree/notebooks/variational_models/2022-02-12-variational-inference.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/prog-ml/prog-ml.github.io/blob/main/notebooks/variational_models/2022-02-12-variational-inference.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#goals">
   Goals:
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#g1-given-probability-distributions-p-and-q-find-the-divergence-measure-of-similarity-between-them">
     G1: Given probability distributions
     <span class="math notranslate nohighlight">
      \(p\)
     </span>
     and
     <span class="math notranslate nohighlight">
      \(q\)
     </span>
     , find the divergence (measure of similarity) between them
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#g2-assuming-p-to-be-fixed-can-we-find-optimum-parameters-of-q-to-make-it-as-close-as-possible-to-p">
     G2: assuming
     <span class="math notranslate nohighlight">
      \(p\)
     </span>
     to be fixed, can we find optimum parameters of
     <span class="math notranslate nohighlight">
      \(q\)
     </span>
     to make it as close as possible to
     <span class="math notranslate nohighlight">
      \(p\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#g3-finding-the-distance-between-two-distributions-of-different-families">
     G3: finding the ‚Äúdistance‚Äù between two distributions of different families
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#g4-optimizing-the-distance-between-two-distributions-of-different-families">
     G4: optimizing the ‚Äúdistance‚Äù between two distributions of different families
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#g5-approximating-the-kl-divergence">
     G5: Approximating the KL-divergence
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#g6-implementing-variational-inference-for-linear-regression">
     G6: Implementing variational inference for linear regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#basic-imports">
   Basic Imports
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#creating-distributions">
   Creating distributions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#creating-p-sim-mathcal-n-1-00-4-00">
     Creating
     <span class="math notranslate nohighlight">
      \(p\sim\mathcal{N}(1.00, 4.00)\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#creating-q-sim-mathcal-n-loc-scale">
     Creating
     <span class="math notranslate nohighlight">
      \(q\sim\mathcal{N}(loc, scale)\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generating-a-few-qs-for-different-location-and-scale-value">
     Generating a few qs for different location and scale value
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimizing-the-kl-divergence-between-q-and-p">
   Optimizing the KL-divergence between q and p
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#animation">
   Animation!
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#finding-the-kl-divergence-for-two-distributions-from-different-families">
   Finding the KL divergence for two distributions from different families
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimizing-the-kl-divergence-for-two-distributions-from-different-families">
   Optimizing the KL divergence for two distributions from different families
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kl-divergence-and-elbo">
     KL-Divergence and ELBO
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-regression">
     Linear Regression
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Variational Inference</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#goals">
   Goals:
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#g1-given-probability-distributions-p-and-q-find-the-divergence-measure-of-similarity-between-them">
     G1: Given probability distributions
     <span class="math notranslate nohighlight">
      \(p\)
     </span>
     and
     <span class="math notranslate nohighlight">
      \(q\)
     </span>
     , find the divergence (measure of similarity) between them
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#g2-assuming-p-to-be-fixed-can-we-find-optimum-parameters-of-q-to-make-it-as-close-as-possible-to-p">
     G2: assuming
     <span class="math notranslate nohighlight">
      \(p\)
     </span>
     to be fixed, can we find optimum parameters of
     <span class="math notranslate nohighlight">
      \(q\)
     </span>
     to make it as close as possible to
     <span class="math notranslate nohighlight">
      \(p\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#g3-finding-the-distance-between-two-distributions-of-different-families">
     G3: finding the ‚Äúdistance‚Äù between two distributions of different families
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#g4-optimizing-the-distance-between-two-distributions-of-different-families">
     G4: optimizing the ‚Äúdistance‚Äù between two distributions of different families
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#g5-approximating-the-kl-divergence">
     G5: Approximating the KL-divergence
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#g6-implementing-variational-inference-for-linear-regression">
     G6: Implementing variational inference for linear regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#basic-imports">
   Basic Imports
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#creating-distributions">
   Creating distributions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#creating-p-sim-mathcal-n-1-00-4-00">
     Creating
     <span class="math notranslate nohighlight">
      \(p\sim\mathcal{N}(1.00, 4.00)\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#creating-q-sim-mathcal-n-loc-scale">
     Creating
     <span class="math notranslate nohighlight">
      \(q\sim\mathcal{N}(loc, scale)\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generating-a-few-qs-for-different-location-and-scale-value">
     Generating a few qs for different location and scale value
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimizing-the-kl-divergence-between-q-and-p">
   Optimizing the KL-divergence between q and p
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#animation">
   Animation!
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#finding-the-kl-divergence-for-two-distributions-from-different-families">
   Finding the KL divergence for two distributions from different families
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimizing-the-kl-divergence-for-two-distributions-from-different-families">
   Optimizing the KL divergence for two distributions from different families
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kl-divergence-and-elbo">
     KL-Divergence and ELBO
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-regression">
     Linear Regression
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="variational-inference">
<h1>Variational Inference<a class="headerlink" href="#variational-inference" title="Permalink to this headline">¬∂</a></h1>
<div class="section" id="goals">
<h2>Goals:<a class="headerlink" href="#goals" title="Permalink to this headline">¬∂</a></h2>
<div class="section" id="g1-given-probability-distributions-p-and-q-find-the-divergence-measure-of-similarity-between-them">
<h3>G1: Given probability distributions <span class="math notranslate nohighlight">\(p\)</span> and <span class="math notranslate nohighlight">\(q\)</span>, find the divergence (measure of similarity) between them<a class="headerlink" href="#g1-given-probability-distributions-p-and-q-find-the-divergence-measure-of-similarity-between-them" title="Permalink to this headline">¬∂</a></h3>
<p>Let us first look at G1. Look at the illustration below. We have a normal distribution <span class="math notranslate nohighlight">\(p\)</span> and two other normal distributions <span class="math notranslate nohighlight">\(q_1\)</span> and <span class="math notranslate nohighlight">\(q_2\)</span>. Which of <span class="math notranslate nohighlight">\(q_1\)</span> and <span class="math notranslate nohighlight">\(q_2\)</span>, would we consider closer to <span class="math notranslate nohighlight">\(p\)</span>? <span class="math notranslate nohighlight">\(q_2\)</span>, right?</p>
<p><img alt="" src="notebooks/variational_models/dkl.png" /></p>
<p>To understand the notion of similarity, we use a metric called the KL-divergence given as <span class="math notranslate nohighlight">\(D_{KL}(a || b)\)</span> where <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> are the two distributions.</p>
<p>For G1, we can say <span class="math notranslate nohighlight">\(q_2\)</span> is closer to <span class="math notranslate nohighlight">\(p\)</span> compared to <span class="math notranslate nohighlight">\(q_1\)</span> as:</p>
<p><span class="math notranslate nohighlight">\(D_{KL}(q_2 || p) \lt D_{KL}(q_1 || p)\)</span></p>
<p>For the above example, we have the values as <span class="math notranslate nohighlight">\(D_{KL}(q_2|| p) = 0.07\)</span> and <span class="math notranslate nohighlight">\(D_{KL}(q_1|| p)= 0.35\)</span></p>
</div>
<div class="section" id="g2-assuming-p-to-be-fixed-can-we-find-optimum-parameters-of-q-to-make-it-as-close-as-possible-to-p">
<h3>G2: assuming <span class="math notranslate nohighlight">\(p\)</span> to be fixed, can we find optimum parameters of <span class="math notranslate nohighlight">\(q\)</span> to make it as close as possible to <span class="math notranslate nohighlight">\(p\)</span><a class="headerlink" href="#g2-assuming-p-to-be-fixed-can-we-find-optimum-parameters-of-q-to-make-it-as-close-as-possible-to-p" title="Permalink to this headline">¬∂</a></h3>
<p>The following GIF shows the process of finding the optimum set of parameters for a normal distribution <span class="math notranslate nohighlight">\(q\)</span> so that it becomes as close as possible to <span class="math notranslate nohighlight">\(p\)</span>. This is equivalent of minimizing <span class="math notranslate nohighlight">\(D_{KL}(q || p)\)</span></p>
<p><img alt="" src="notebooks/variational_models/kl_qp.gif" /></p>
<p>The following GIF shows the above but for a two-dimensional distribution.</p>
<p><img alt="" src="notebooks/variational_models/kl_qp_2.gif" /></p>
</div>
<div class="section" id="g3-finding-the-distance-between-two-distributions-of-different-families">
<h3>G3: finding the ‚Äúdistance‚Äù between two distributions of different families<a class="headerlink" href="#g3-finding-the-distance-between-two-distributions-of-different-families" title="Permalink to this headline">¬∂</a></h3>
<p>The below image shows the KL-divergence between distribution 1 (mixture of Gaussians) and distribution 2 (Gaussian)</p>
<p><img alt="" src="notebooks/variational_models/dkl-different.png" /></p>
</div>
<div class="section" id="g4-optimizing-the-distance-between-two-distributions-of-different-families">
<h3>G4: optimizing the ‚Äúdistance‚Äù between two distributions of different families<a class="headerlink" href="#g4-optimizing-the-distance-between-two-distributions-of-different-families" title="Permalink to this headline">¬∂</a></h3>
<p>The below GIF shows the optimization of the KL-divergence between distribution 1 (mixture of Gaussians) and distribution 2 (Gaussian)</p>
<p><img alt="" src="notebooks/variational_models/kl_qp_mg.gif" /></p>
</div>
<div class="section" id="g5-approximating-the-kl-divergence">
<h3>G5: Approximating the KL-divergence<a class="headerlink" href="#g5-approximating-the-kl-divergence" title="Permalink to this headline">¬∂</a></h3>
</div>
<div class="section" id="g6-implementing-variational-inference-for-linear-regression">
<h3>G6: Implementing variational inference for linear regression<a class="headerlink" href="#g6-implementing-variational-inference-for-linear-regression" title="Permalink to this headline">¬∂</a></h3>
</div>
</div>
<div class="section" id="basic-imports">
<h2>Basic Imports<a class="headerlink" href="#basic-imports" title="Permalink to this headline">¬∂</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">dist</span> <span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">distributions</span>

<span class="n">sns</span><span class="o">.</span><span class="n">reset_defaults</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">context</span><span class="o">=</span><span class="s2">&quot;talk&quot;</span><span class="p">,</span> <span class="n">font_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format=&#39;retina&#39;
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="creating-distributions">
<h2>Creating distributions<a class="headerlink" href="#creating-distributions" title="Permalink to this headline">¬∂</a></h2>
<div class="section" id="creating-p-sim-mathcal-n-1-00-4-00">
<h3>Creating <span class="math notranslate nohighlight">\(p\sim\mathcal{N}(1.00, 4.00)\)</span><a class="headerlink" href="#creating-p-sim-mathcal-n-1-00-4-00" title="Permalink to this headline">¬∂</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">z_values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">prob_values_p</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">z_values</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_values</span><span class="p">,</span> <span class="n">prob_values_p</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$p\sim\mathcal</span><span class="si">{N}</span><span class="s2">(1.00, 4.00)$&quot;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;PDF&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;PDF&#39;)
</pre></div>
</div>
<img alt="../../_images/2022-02-12-variational-inference_6_1.png" src="../../_images/2022-02-12-variational-inference_6_1.png" />
</div>
</div>
</div>
<div class="section" id="creating-q-sim-mathcal-n-loc-scale">
<h3>Creating <span class="math notranslate nohighlight">\(q\sim\mathcal{N}(loc, scale)\)</span><a class="headerlink" href="#creating-q-sim-mathcal-n-loc-scale" title="Permalink to this headline">¬∂</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_q</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="generating-a-few-qs-for-different-location-and-scale-value">
<h3>Generating a few qs for different location and scale value<a class="headerlink" href="#generating-a-few-qs-for-different-location-and-scale-value" title="Permalink to this headline">¬∂</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">q</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">q</span><span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">create_q</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>

<span class="k">for</span> <span class="n">loc</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]:</span>
    <span class="k">for</span> <span class="n">scale</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]:</span>
        <span class="n">q</span><span class="p">[(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">)]</span> <span class="o">=</span> <span class="n">create_q</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">loc</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="n">scale</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_values</span><span class="p">,</span> <span class="n">prob_values_p</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$p\sim\mathcal</span><span class="si">{N}</span><span class="s2">(1.00, 4.00)$&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">z_values</span><span class="p">,</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">create_q</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">)</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">z_values</span><span class="p">)),</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$q_1\sim\mathcal</span><span class="si">{N}</span><span class="s2">(0.00, 2.00)$&quot;</span><span class="p">,</span>
    <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">z_values</span><span class="p">,</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">create_q</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">)</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">z_values</span><span class="p">)),</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$q_2\sim\mathcal</span><span class="si">{N}</span><span class="s2">(1.00, 3.00)$&quot;</span><span class="p">,</span>
    <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;-.&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.04</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">borderaxespad</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;PDF&quot;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span>
    <span class="s2">&quot;dkl.png&quot;</span><span class="p">,</span>
    <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/2022-02-12-variational-inference_11_0.png" src="../../_images/2022-02-12-variational-inference_11_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#### Computing KL-divergence</span>

<span class="n">q_0_2_dkl</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">kl_divergence</span><span class="p">(</span><span class="n">create_q</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">),</span> <span class="n">p</span><span class="p">)</span>
<span class="n">q_1_3_dkl</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">kl_divergence</span><span class="p">(</span><span class="n">create_q</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">),</span> <span class="n">p</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;D_KL (q(0, 2)||p) = </span><span class="si">{</span><span class="n">q_0_2_dkl</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;D_KL (q(1, 3)||p) = </span><span class="si">{</span><span class="n">q_1_3_dkl</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>D_KL (q(0, 2)||p) = 0.35
D_KL (q(1, 3)||p) = 0.07
</pre></div>
</div>
</div>
</div>
<p>As mentioned earlier, clearly, <span class="math notranslate nohighlight">\(q_2\sim\mathcal{N}(1.00, 3.00)\)</span> seems closer to <span class="math notranslate nohighlight">\(p\)</span></p>
</div>
</div>
<div class="section" id="optimizing-the-kl-divergence-between-q-and-p">
<h2>Optimizing the KL-divergence between q and p<a class="headerlink" href="#optimizing-the-kl-divergence-between-q-and-p" title="Permalink to this headline">¬∂</a></h2>
<p>We could create a grid of (loc, scale) pairs and find the best, as shown below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_values</span><span class="p">,</span> <span class="n">prob_values_p</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$p\sim\mathcal</span><span class="si">{N}</span><span class="s2">(1.00, 4.00)$&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>


<span class="k">for</span> <span class="n">loc</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]:</span>
    <span class="k">for</span> <span class="n">scale</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]:</span>
        <span class="n">q_d</span> <span class="o">=</span> <span class="n">q</span><span class="p">[(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">)]</span>
        <span class="n">kl_d</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">kl_divergence</span><span class="p">(</span><span class="n">q</span><span class="p">[(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">)],</span> <span class="n">p</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
            <span class="n">z_values</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">q_d</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">z_values</span><span class="p">)),</span>
            <span class="n">label</span><span class="o">=</span><span class="sa">rf</span><span class="s2">&quot;$q\sim\mathcal</span><span class="se">{{</span><span class="s2">N</span><span class="se">}}</span><span class="s2">(</span><span class="si">{</span><span class="n">loc</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">scale</span><span class="si">}</span><span class="s2">)$&quot;</span>
            <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="o">+</span> <span class="sa">rf</span><span class="s2">&quot;$D_</span><span class="se">{{</span><span class="s2">KL</span><span class="se">}}</span><span class="s2">(q||p)$ = </span><span class="si">{</span><span class="n">kl_d</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.04</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">borderaxespad</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;PDF&quot;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/2022-02-12-variational-inference_15_0.png" src="../../_images/2022-02-12-variational-inference_15_0.png" />
</div>
</div>
<p>Or, we could use continuous optimization to find the best loc and scale parameters for q.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">8.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="n">loc_array</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">scale_array</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">loss_array</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">401</span><span class="p">):</span>
    <span class="n">scale_softplus</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">F</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span>

    <span class="n">to_learn</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale_softplus</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">kl_divergence</span><span class="p">(</span><span class="n">to_learn</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
    <span class="n">loss_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">loc_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">to_learn</span><span class="o">.</span><span class="n">loc</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">scale_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">to_learn</span><span class="o">.</span><span class="n">scale</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Iteration: </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">, Loc: </span><span class="si">{</span><span class="n">loc</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">, Scale: </span><span class="si">{</span><span class="n">scale_softplus</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Iteration: 0, Loss: 2.73, Loc: 8.00, Scale: 0.74
Iteration: 100, Loss: 0.26, Loc: 3.75, Scale: 3.46
Iteration: 200, Loss: 0.01, Loc: 1.68, Scale: 3.98
Iteration: 300, Loss: 0.00, Loc: 1.10, Scale: 4.00
Iteration: 400, Loss: 0.00, Loc: 1.01, Scale: 4.00
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">scale_array</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">loc_array</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">loss_array</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x13d709d30&gt;]
</pre></div>
</div>
<img alt="../../_images/2022-02-12-variational-inference_18_1.png" src="../../_images/2022-02-12-variational-inference_18_1.png" />
</div>
</div>
<p>After training, we are able to recover the scale and loc very close to that of <span class="math notranslate nohighlight">\(p\)</span></p>
</div>
<div class="section" id="animation">
<h2>Animation!<a class="headerlink" href="#animation" title="Permalink to this headline">¬∂</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">animation</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">tight_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>    


<span class="k">def</span> <span class="nf">animate</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_values</span><span class="p">,</span> <span class="n">prob_values_p</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$p\sim\mathcal</span><span class="si">{N}</span><span class="s2">(1.00, 4.00)$&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">to_learn_q</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="n">loc_array</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale_array</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
        <span class="n">z_values</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">to_learn_q</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">z_values</span><span class="p">)),</span>
        <span class="n">label</span><span class="o">=</span><span class="sa">rf</span><span class="s2">&quot;$q\sim \mathcal</span><span class="se">{{</span><span class="s2">N</span><span class="se">}}</span><span class="s2">(</span><span class="si">{</span><span class="n">loc</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">scale</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">)$&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">rf</span><span class="s2">&quot;Iteration: </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">, $D_</span><span class="se">{{</span><span class="s2">KL</span><span class="se">}}</span><span class="s2">(q||p)$: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">borderaxespad</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">((</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">15</span><span class="p">))</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;PDF&quot;</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>


<span class="n">ani</span> <span class="o">=</span> <span class="n">animation</span><span class="o">.</span><span class="n">FuncAnimation</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">animate</span><span class="p">,</span> <span class="n">frames</span><span class="o">=</span><span class="mi">350</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ani</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;kl_qp.gif&quot;</span><span class="p">,</span> <span class="n">writer</span><span class="o">=</span><span class="s2">&quot;imagemagick&quot;</span><span class="p">,</span> <span class="n">fps</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;Figure size 432x288 with 0 Axes&gt;
</pre></div>
</div>
</div>
</div>
<p><img alt="" src="notebooks/variational_models/kl_qp.gif" /></p>
</div>
<div class="section" id="finding-the-kl-divergence-for-two-distributions-from-different-families">
<h2>Finding the KL divergence for two distributions from different families<a class="headerlink" href="#finding-the-kl-divergence-for-two-distributions-from-different-families" title="Permalink to this headline">¬∂</a></h2>
<p>Let us rework our example with <code class="docutils literal notranslate"><span class="pre">p</span></code> coming from  a mixture of Gaussian distribution and <code class="docutils literal notranslate"><span class="pre">q</span></code> being Normal.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">p_s</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">MixtureSameFamily</span><span class="p">(</span>
    <span class="n">mixture_distribution</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])),</span>
    <span class="n">component_distribution</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span>
        <span class="n">loc</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">scale</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>  <span class="c1"># One for each component.</span>
    <span class="p">),</span>
<span class="p">)</span>  

<span class="n">p_s</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MixtureSameFamily(
  Categorical(probs: torch.Size([2]), logits: torch.Size([2])),
  Normal(loc: torch.Size([2]), scale: torch.Size([2])))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_values</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">p_s</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">z_values</span><span class="p">)))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/2022-02-12-variational-inference_26_0.png" src="../../_images/2022-02-12-variational-inference_26_0.png" />
</div>
</div>
<p>Let us create two Normal distributions q_1 and q_2 and plot them to see which looks closer to p_s.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">q_1</span> <span class="o">=</span> <span class="n">create_q</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">q_2</span> <span class="o">=</span> <span class="n">create_q</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prob_values_p_s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">p_s</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">z_values</span><span class="p">))</span>
<span class="n">prob_values_q_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">q_1</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">z_values</span><span class="p">))</span>
<span class="n">prob_values_q_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">q_2</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">z_values</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_values</span><span class="p">,</span> <span class="n">prob_values_p_s</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;MOG&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_values</span><span class="p">,</span> <span class="n">prob_values_q_1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$q_1\sim\mathcal</span><span class="si">{N}</span><span class="s2"> (3, 1.0)$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_values</span><span class="p">,</span> <span class="n">prob_values_q_2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$q_2\sim\mathcal</span><span class="si">{N}</span><span class="s2"> (3, 4.5)$&quot;</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;PDF&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span>
    <span class="s2">&quot;dkl-different.png&quot;</span><span class="p">,</span>
    <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/2022-02-12-variational-inference_29_0.png" src="../../_images/2022-02-12-variational-inference_29_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">kl_divergence</span><span class="p">(</span><span class="n">q_1</span><span class="p">,</span> <span class="n">p_s</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">NotImplementedError</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;KL divergence not implemented between </span><span class="si">{</span><span class="n">q_1</span><span class="o">.</span><span class="vm">__class__</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">p_s</span><span class="o">.</span><span class="vm">__class__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>KL divergence not implemented between &lt;class &#39;torch.distributions.normal.Normal&#39;&gt; and &lt;class &#39;torch.distributions.mixture_same_family.MixtureSameFamily&#39;&gt;
</pre></div>
</div>
</div>
</div>
<p>As we see above, we can not compute the KL divergence directly. The core idea would now be to leverage the Monte Carlo sampling and generating the expectation. The following function does that.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">kl_via_sampling</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">100000</span><span class="p">):</span>
    <span class="c1"># Get samples from q</span>
    <span class="n">sample_set</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">sample</span><span class="p">([</span><span class="n">n_samples</span><span class="p">])</span>
    <span class="c1"># Use the definition of KL-divergence</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">sample_set</span><span class="p">)</span> <span class="o">-</span> <span class="n">p</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">sample_set</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dist</span><span class="o">.</span><span class="n">kl_divergence</span><span class="p">(</span><span class="n">q_1</span><span class="p">,</span> <span class="n">q_2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(1.0288)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">kl_via_sampling</span><span class="p">(</span><span class="n">q_1</span><span class="p">,</span> <span class="n">q_2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(1.0268)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">kl_via_sampling</span><span class="p">(</span><span class="n">q_1</span><span class="p">,</span> <span class="n">p_s</span><span class="p">),</span> <span class="n">kl_via_sampling</span><span class="p">(</span><span class="n">q_2</span><span class="p">,</span> <span class="n">p_s</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor(9.4963), tensor(45.4601))
</pre></div>
</div>
</div>
</div>
<p>As we can see from KL divergence calculations, <code class="docutils literal notranslate"><span class="pre">q_1</span></code> is closer to our Gaussian mixture distribution.</p>
</div>
<div class="section" id="optimizing-the-kl-divergence-for-two-distributions-from-different-families">
<h2>Optimizing the KL divergence for two distributions from different families<a class="headerlink" href="#optimizing-the-kl-divergence-for-two-distributions-from-different-families" title="Permalink to this headline">¬∂</a></h2>
<p>We saw that we can calculate the KL divergence between two different distribution families via sampling. But, as we did earlier, will we be able to optimize the parameters of our target surrogate distribution? The answer is No! As we have introduced sampling. However, there is still a way ‚Äì by reparameterization!</p>
<p>Our surrogate q in this case is parameterized by <code class="docutils literal notranslate"><span class="pre">loc</span></code> and <code class="docutils literal notranslate"><span class="pre">scale</span></code>. The key idea here is to generate samples from a standard normal distribution (loc=0, scale=1) and then apply an affine transformation on the generated samples to get the samples generated from q. See my other post on sampling from normal distribution to understand this better.</p>
<p>The loss can now be thought of as a function of <code class="docutils literal notranslate"><span class="pre">loc</span></code> and <code class="docutils literal notranslate"><span class="pre">scale</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_samples</span> <span class="o">=</span> <span class="mi">1000</span>


<span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">):</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">)</span>
    <span class="n">std_normal</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="n">sample_set</span> <span class="o">=</span> <span class="n">std_normal</span><span class="o">.</span><span class="n">sample</span><span class="p">([</span><span class="n">n_samples</span><span class="p">])</span>
    <span class="n">sample_set</span> <span class="o">=</span> <span class="n">loc</span> <span class="o">+</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">sample_set</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">sample_set</span><span class="p">)</span> <span class="o">-</span> <span class="n">p_s</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">sample_set</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Having defined the loss above, we can now optimize <code class="docutils literal notranslate"><span class="pre">loc</span></code> and <code class="docutils literal notranslate"><span class="pre">scale</span></code> to minimize the KL-divergence.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">8.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="n">loc_array</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">scale_array</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">loss_array</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">401</span><span class="p">):</span>
    <span class="n">scale_softplus</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">F</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span>

    <span class="n">to_learn</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale_softplus</span><span class="p">)</span>
    <span class="n">loss_value</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale_softplus</span><span class="p">)</span>
    <span class="n">loss_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_value</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">loc_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">to_learn</span><span class="o">.</span><span class="n">loc</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">scale_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">to_learn</span><span class="o">.</span><span class="n">scale</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="n">loss_value</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Iteration: </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">loss_value</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">, Loc: </span><span class="si">{</span><span class="n">loc</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">, Scale: </span><span class="si">{</span><span class="n">scale_softplus</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Iteration: 0, Loss: 99.76, Loc: 8.00, Scale: 0.74
Iteration: 100, Loss: 15.53, Loc: 3.75, Scale: 0.49
Iteration: 200, Loss: 1.57, Loc: 1.67, Scale: 0.51
Iteration: 300, Loss: 0.33, Loc: 0.96, Scale: 0.70
Iteration: 400, Loss: 0.08, Loc: 0.62, Scale: 0.74
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">q_s</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale_softplus</span><span class="p">)</span>
<span class="n">q_s</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Normal(loc: 0.6216101050376892, scale: 0.739622175693512)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prob_values_p_s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">p_s</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">z_values</span><span class="p">))</span>
<span class="n">prob_values_q_s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">q_s</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">z_values</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_values</span><span class="p">,</span> <span class="n">prob_values_p_s</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;p&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_values</span><span class="p">,</span> <span class="n">prob_values_q_s</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;q&quot;</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;PDF&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;PDF&#39;)
</pre></div>
</div>
<img alt="../../_images/2022-02-12-variational-inference_43_1.png" src="../../_images/2022-02-12-variational-inference_43_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prob_values_p_s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">p_s</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">z_values</span><span class="p">))</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">tight_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
<span class="n">n_iter</span> <span class="o">=</span> <span class="mi">300</span>

<span class="k">def</span> <span class="nf">a</span><span class="p">(</span><span class="n">iteration</span><span class="p">):</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
    <span class="n">loc</span> <span class="o">=</span> <span class="n">loc_array</span><span class="p">[</span><span class="n">iteration</span><span class="p">]</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">scale_array</span><span class="p">[</span><span class="n">iteration</span><span class="p">]</span>
    <span class="n">q_s</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">)</span>

    <span class="n">prob_values_q_s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">q_s</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">z_values</span><span class="p">))</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_values</span><span class="p">,</span> <span class="n">prob_values_p_s</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;p&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_values</span><span class="p">,</span> <span class="n">prob_values_q_s</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;q&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Iteration </span><span class="si">{</span><span class="n">iteration</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">loss_array</span><span class="p">[</span><span class="n">iteration</span><span class="p">]</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">((</span><span class="o">-</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">ani_mg</span> <span class="o">=</span> <span class="n">animation</span><span class="o">.</span><span class="n">FuncAnimation</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">frames</span><span class="o">=</span><span class="n">n_iter</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">loc_array</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;loc&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">scale_array</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;scale&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Iterations&quot;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x140988be0&gt;
</pre></div>
</div>
<img alt="../../_images/2022-02-12-variational-inference_45_1.png" src="../../_images/2022-02-12-variational-inference_45_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ani_mg</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;kl_qp_mg.gif&quot;</span><span class="p">,</span> <span class="n">writer</span><span class="o">=</span><span class="s2">&quot;imagemagick&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><img alt="" src="notebooks/variational_models/kl_qp_mg.gif" /></p>
<div class="section" id="kl-divergence-and-elbo">
<h3>KL-Divergence and ELBO<a class="headerlink" href="#kl-divergence-and-elbo" title="Permalink to this headline">¬∂</a></h3>
<p>Let us consider linear regression. We have parameters <span class="math notranslate nohighlight">\(\theta \in R^D\)</span> and we define a prior over them. Let us assume we define prior <span class="math notranslate nohighlight">\(p(\theta)\sim \mathcal{N_D} (\mu, \Sigma)\)</span>. Now, given our dataset <span class="math notranslate nohighlight">\(D = \{X, y\}\)</span> and a parameter vector <span class="math notranslate nohighlight">\(\theta\)</span>, we can deifine our likelihood as <span class="math notranslate nohighlight">\(p(D|\theta)\)</span> or <span class="math notranslate nohighlight">\(p(y|X, \theta) = \prod_{i=1}^{n} p(y_i|x_i, \theta) = \prod_{i=1}^{n} \mathcal{N}(y_i|x_i^T\theta, \sigma^2) \)</span></p>
<p>As per Bayes rule, we can obtain the posterior over <span class="math notranslate nohighlight">\(\theta\)</span> as:</p>
<p><span class="math notranslate nohighlight">\(p(\theta|D) = \dfrac{p(D|\theta)p(\theta)}{p(D)}\)</span></p>
<p>Now, in general <span class="math notranslate nohighlight">\(p(D)\)</span> is hard to compute.</p>
<p>So, in variational inference, our aim is to use a surrogate distribution <span class="math notranslate nohighlight">\(q(\theta)\)</span> such that it is very close to <span class="math notranslate nohighlight">\(p(\theta|D)\)</span>. We do so by minimizing the KL divergence between <span class="math notranslate nohighlight">\(q(\theta)\)</span> and <span class="math notranslate nohighlight">\(p(\theta|D)\)</span>.</p>
<p>Aim: $<span class="math notranslate nohighlight">\(q^*(\theta) = \underset{q(\theta) \in \mathcal{Q}}{\mathrm{argmin~}} D_{KL}[q(\theta)||p(\theta|D)]\)</span>$</p>
<p>Now,  $<span class="math notranslate nohighlight">\(D_{KL}[q(\theta)||p(\theta|D)] = \mathbb{E}_{q(\theta)}[\log\frac{q(\theta)}{p(\theta|D)}]\)</span><span class="math notranslate nohighlight">\(
Now,  \)</span><span class="math notranslate nohighlight">\( = \mathbb{E}_{q(\theta)}[\log\frac{q(\theta)p(D)}{p(\theta, D)}]\)</span><span class="math notranslate nohighlight">\(
Now,  \)</span><span class="math notranslate nohighlight">\( = \mathbb{E}_{q(\theta)}[\log q(\theta)]- \mathbb{E}_{q(\theta)}[\log p(\theta, D)] + \mathbb{E}_{q(\theta)}[\log p(D)] \)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(= \mathbb{E}_{q(\theta)}[\log q(\theta)]- \mathbb{E}_{q(\theta)}[\log p(\theta, D)] + \log p(D) \)</span>$</p>
<p>Now, <span class="math notranslate nohighlight">\(p(D) \in \{0, 1\}\)</span>. Thus, <span class="math notranslate nohighlight">\(\log p(D) \in \{-\infty, 0 \}\)</span></p>
<p>Now, let us look at the quantities:</p>
<div class="math notranslate nohighlight">
\[\underbrace{D_{KL}[q(\theta)||p(\theta|D)]}_{\geq 0} = \underbrace{\mathbb{E}_{q(\theta)}[\log q(\theta)]- \mathbb{E}_{q(\theta)}[\log p(\theta, D)]}_{-\text{ELBO(q)}} +  \underbrace{\log p(D)}_{\leq 0}\]</div>
<p>Thus, we know that <span class="math notranslate nohighlight">\(\log p(D) \geq \text{ELBO(q)}\)</span></p>
<p>Thus, finally we can rewrite the optimisation from</p>
<div class="math notranslate nohighlight">
\[q^*(\theta) = \underset{q(\theta) \in \mathcal{Q}}{\mathrm{argmin~}} D_{KL}[q(\theta)||p(\theta|D)]\]</div>
<p>to</p>
<div class="math notranslate nohighlight">
\[q^*(\theta) = \underset{q(\theta) \in \mathcal{Q}}{\mathrm{argmax~}} \text{ELBO(q)}\]</div>
<p>Now, given our linear regression problem setup, we want to maximize the ELBO.</p>
<p>We can do so by the following. As a simple example, let us assume <span class="math notranslate nohighlight">\(\theta \in R^2\)</span></p>
<ul class="simple">
<li><p>Assume some q. Say, a Normal distribution. So, <span class="math notranslate nohighlight">\(q\sim \mathcal{N}_2\)</span></p></li>
<li><p>Draw samples from q. Say N samples.</p></li>
<li><p>Initilize ELBO = 0.0</p></li>
<li><p>For each sample:</p>
<ul>
<li><p>Let us assume drawn sample is <span class="math notranslate nohighlight">\([\theta_1, \theta_2]^T\)</span></p></li>
<li><p>Compute log_prob of prior on <span class="math notranslate nohighlight">\([\theta_1, \theta_2]^T\)</span> or <code class="docutils literal notranslate"><span class="pre">lp</span> <span class="pre">=</span> <span class="pre">p.log_prob(Œ∏1,</span> <span class="pre">Œ∏2)</span></code></p></li>
<li><p>Compute log_prob of likelihood on <span class="math notranslate nohighlight">\([\theta_1, \theta_2]^T\)</span> or <code class="docutils literal notranslate"><span class="pre">ll</span> <span class="pre">=</span> <span class="pre">l.log_prob(Œ∏1,</span> <span class="pre">Œ∏2)</span></code></p></li>
<li><p>Compute log_prob of q on <span class="math notranslate nohighlight">\([\theta_1, \theta_2]^T\)</span> or <code class="docutils literal notranslate"><span class="pre">lq</span> <span class="pre">=</span> <span class="pre">q.log_prob(Œ∏1,</span> <span class="pre">Œ∏2)</span></code></p></li>
<li><p>ELBO = ELBO + (ll+lp-q)</p></li>
</ul>
</li>
<li><p>Return ELBO/N</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prior</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mf">5.</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">samples</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">sample</span><span class="p">([</span><span class="mi">1000</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">surrogate_sample</span><span class="p">(</span><span class="n">mu</span><span class="p">):</span>
    <span class="n">std_normal</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>
    <span class="n">sample_std_normal</span>  <span class="o">=</span> <span class="n">std_normal</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">sample_std_normal</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">samples_from_surrogate</span> <span class="o">=</span> <span class="n">surrogate_sample</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">samples_from_surrogate</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(2.7988, grad_fn=&lt;AddBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">logprob_prior</span><span class="p">(</span><span class="n">mu</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">prior</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>

<span class="n">lp</span> <span class="o">=</span> <span class="n">logprob_prior</span><span class="p">(</span><span class="n">samples_from_surrogate</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">log_likelihood</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">samples</span><span class="p">):</span>
    <span class="n">di</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">di</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">samples</span><span class="p">))</span>

<span class="n">ll</span> <span class="o">=</span> <span class="n">log_likelihood</span><span class="p">(</span><span class="n">samples_from_surrogate</span><span class="p">,</span> <span class="n">samples</span><span class="p">)</span>

<span class="n">ls</span> <span class="o">=</span> <span class="n">surrogate</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">samples_from_surrogate</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">elbo_loss</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">data_samples</span><span class="p">):</span>
    <span class="n">samples_from_surrogate</span> <span class="o">=</span> <span class="n">surrogate_sample</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>
    <span class="n">lp</span> <span class="o">=</span> <span class="n">logprob_prior</span><span class="p">(</span><span class="n">samples_from_surrogate</span><span class="p">)</span>
    <span class="n">ll</span> <span class="o">=</span> <span class="n">log_likelihood</span><span class="p">(</span><span class="n">samples_from_surrogate</span><span class="p">,</span> <span class="n">data_samples</span><span class="p">)</span>
    <span class="n">ls</span> <span class="o">=</span> <span class="n">surrogate</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">samples_from_surrogate</span><span class="p">)</span>

    <span class="k">return</span> <span class="o">-</span><span class="n">lp</span> <span class="o">-</span> <span class="n">ll</span> <span class="o">+</span> <span class="n">ls</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">loc_array</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">loss_array</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">mu</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2000</span><span class="p">):</span>
    <span class="n">loss_val</span> <span class="o">=</span> <span class="n">elbo_loss</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">samples</span><span class="p">)</span>
    <span class="n">loss_val</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">loc_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mu</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">loss_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_val</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Iteration: </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">loss_val</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">, Loc: </span><span class="si">{</span><span class="n">mu</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">0.3f</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Iteration: 0, Loss: 11693.85, Loc: 1.000
Iteration: 100, Loss: 2550.90, Loc: 2.744
Iteration: 200, Loss: 2124.30, Loc: 3.871
Iteration: 300, Loss: 2272.48, Loc: 4.582
Iteration: 400, Loss: 2025.17, Loc: 4.829
Iteration: 500, Loss: 1434.45, Loc: 5.079
Iteration: 600, Loss: 1693.33, Loc: 5.007
Iteration: 700, Loss: 1495.89, Loc: 4.957
Iteration: 800, Loss: 2698.28, Loc: 5.149
Iteration: 900, Loss: 2819.85, Loc: 5.117
Iteration: 1000, Loss: 1491.79, Loc: 5.112
Iteration: 1100, Loss: 1767.87, Loc: 4.958
Iteration: 1200, Loss: 1535.30, Loc: 4.988
Iteration: 1300, Loss: 1458.61, Loc: 4.949
Iteration: 1400, Loss: 1400.21, Loc: 4.917
Iteration: 1500, Loss: 2613.42, Loc: 5.073
Iteration: 1600, Loss: 1411.46, Loc: 4.901
Iteration: 1700, Loss: 1587.94, Loc: 5.203
Iteration: 1800, Loss: 1461.40, Loc: 5.011
Iteration: 1900, Loss: 1504.93, Loc: 5.076
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">loss_array</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x14069f5e0&gt;]
</pre></div>
</div>
<img alt="../../_images/2022-02-12-variational-inference_57_1.png" src="../../_images/2022-02-12-variational-inference_57_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numpy.lib.stride_tricks</span> <span class="kn">import</span> <span class="n">sliding_window_view</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">sliding_window_view</span><span class="p">(</span><span class="n">loss_array</span><span class="p">,</span> <span class="n">window_shape</span> <span class="o">=</span> <span class="mi">10</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x14062c250&gt;]
</pre></div>
</div>
<img alt="../../_images/2022-02-12-variational-inference_58_1.png" src="../../_images/2022-02-12-variational-inference_58_1.png" />
</div>
</div>
</div>
<div class="section" id="linear-regression">
<h3>Linear Regression<a class="headerlink" href="#linear-regression" title="Permalink to this headline">¬∂</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">true_theta_0</span> <span class="o">=</span> <span class="mf">3.</span>
<span class="n">true_theta_1</span> <span class="o">=</span> <span class="mf">4.</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">true_theta_0</span> <span class="o">+</span> <span class="n">true_theta_1</span><span class="o">*</span><span class="n">x</span>
<span class="n">y_noisy</span> <span class="o">=</span> <span class="n">y_true</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">std</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_true</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_noisy</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.collections.PathCollection at 0x1407aaac0&gt;
</pre></div>
</div>
<img alt="../../_images/2022-02-12-variational-inference_60_1.png" src="../../_images/2022-02-12-variational-inference_60_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">x_dash</span><span class="nd">@theta_prior</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Fit&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_noisy</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Data&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x14064e850&gt;
</pre></div>
</div>
<img alt="../../_images/2022-02-12-variational-inference_62_1.png" src="../../_images/2022-02-12-variational-inference_62_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">theta_prior</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">MultivariateNormal</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]),</span> <span class="n">covariance_matrix</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">likelihood</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">x_dash</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">x_dash</span><span class="nd">@theta</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">likelihood</span><span class="p">(</span><span class="n">theta_prior</span><span class="o">.</span><span class="n">sample</span><span class="p">(),</span> <span class="n">x</span><span class="p">,</span> <span class="n">y_noisy</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(-3558.0769)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">surrogate_mvn</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">MultivariateNormal</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="n">loc</span><span class="p">,</span> <span class="n">covariance_matrix</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="n">surrogate_mvn</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MultivariateNormal(loc: torch.Size([2]), covariance_matrix: torch.Size([2, 2]))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">surrogate_mvn</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([-1.1585,  2.6212])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">surrogate_sample_mvn</span><span class="p">(</span><span class="n">loc</span><span class="p">):</span>
    <span class="n">std_normal_mvn</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">MultivariateNormal</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">loc</span><span class="p">),</span> <span class="n">covariance_matrix</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">loc</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="n">sample_std_normal</span>  <span class="o">=</span> <span class="n">std_normal_mvn</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">loc</span> <span class="o">+</span> <span class="n">sample_std_normal</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">elbo_loss</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">samples_from_surrogate_mvn</span> <span class="o">=</span> <span class="n">surrogate_sample_mvn</span><span class="p">(</span><span class="n">loc</span><span class="p">)</span>
    <span class="n">lp</span> <span class="o">=</span> <span class="n">theta_prior</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">samples_from_surrogate_mvn</span><span class="p">)</span>
    <span class="n">ll</span> <span class="o">=</span> <span class="n">likelihood</span><span class="p">(</span><span class="n">samples_from_surrogate_mvn</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y_noisy</span><span class="p">)</span>
    <span class="n">ls</span> <span class="o">=</span> <span class="n">surrogate_mvn</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">samples_from_surrogate_mvn</span><span class="p">)</span>

    <span class="k">return</span> <span class="o">-</span><span class="n">lp</span> <span class="o">-</span> <span class="n">ll</span> <span class="o">+</span> <span class="n">ls</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loc</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_noisy</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(torch.Size([2]), torch.Size([100]), torch.Size([100]))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">elbo_loss</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y_noisy</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(2850.3154, grad_fn=&lt;AddBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="n">loc_array</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">loss_array</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">loc</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
    <span class="n">loss_val</span> <span class="o">=</span> <span class="n">elbo_loss</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y_noisy</span><span class="p">)</span>
    <span class="n">loss_val</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">loc_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mu</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">loss_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_val</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Iteration: </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">loss_val</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">, Loc: </span><span class="si">{</span><span class="n">loc</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Iteration: 0, Loss: 5479.97, Loc: tensor([-1.,  1.], requires_grad=True)
Iteration: 1000, Loss: 566.63, Loc: tensor([2.9970, 4.0573], requires_grad=True)
Iteration: 2000, Loss: 362.19, Loc: tensor([2.9283, 3.9778], requires_grad=True)
Iteration: 3000, Loss: 231.23, Loc: tensor([2.8845, 4.1480], requires_grad=True)
Iteration: 4000, Loss: 277.94, Loc: tensor([2.9284, 3.9904], requires_grad=True)
Iteration: 5000, Loss: 1151.51, Loc: tensor([2.9620, 4.0523], requires_grad=True)
Iteration: 6000, Loss: 582.19, Loc: tensor([2.8003, 4.0540], requires_grad=True)
Iteration: 7000, Loss: 178.48, Loc: tensor([2.8916, 3.9968], requires_grad=True)
Iteration: 8000, Loss: 274.76, Loc: tensor([3.0807, 4.1957], requires_grad=True)
Iteration: 9000, Loss: 578.37, Loc: tensor([2.9830, 4.0174], requires_grad=True)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">learnt_surrogate</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">MultivariateNormal</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="n">loc</span><span class="p">,</span> <span class="n">covariance_matrix</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_samples_surrogate</span> <span class="o">=</span> <span class="n">x_dash</span><span class="nd">@learnt_surrogate</span><span class="o">.</span><span class="n">sample</span><span class="p">([</span><span class="mi">500</span><span class="p">])</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_samples_surrogate</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.02</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_noisy</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.collections.PathCollection at 0x144709a90&gt;
</pre></div>
</div>
<img alt="../../_images/2022-02-12-variational-inference_73_1.png" src="../../_images/2022-02-12-variational-inference_73_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_dash</span><span class="nd">@learnt_surrogate</span><span class="o">.</span><span class="n">loc</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
<span class="n">theta_sd</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">learnt_surrogate</span><span class="o">.</span><span class="n">covariance_matrix</span><span class="p">)</span>


<span class="c1">#y_samples_surrogate = x_dash@learnt_surrogate.loc.t()</span>
<span class="c1">#plt.plot(x, y_samples_surrogate, alpha = 0.02, color=&#39;k&#39;);</span>
<span class="c1">#plt.scatter(x, y_noisy, s=20, alpha=0.5)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([-1.6542e+01, -1.6148e+01, -1.5754e+01, -1.5360e+01, -1.4966e+01,
        -1.4572e+01, -1.4178e+01, -1.3784e+01, -1.3390e+01, -1.2996e+01,
        -1.2602e+01, -1.2208e+01, -1.1814e+01, -1.1420e+01, -1.1026e+01,
        -1.0632e+01, -1.0238e+01, -9.8441e+00, -9.4501e+00, -9.0561e+00,
        -8.6621e+00, -8.2681e+00, -7.8741e+00, -7.4801e+00, -7.0860e+00,
        -6.6920e+00, -6.2980e+00, -5.9040e+00, -5.5100e+00, -5.1160e+00,
        -4.7220e+00, -4.3280e+00, -3.9340e+00, -3.5400e+00, -3.1460e+00,
        -2.7520e+00, -2.3579e+00, -1.9639e+00, -1.5699e+00, -1.1759e+00,
        -7.8191e-01, -3.8790e-01,  6.1054e-03,  4.0011e-01,  7.9412e-01,
         1.1881e+00,  1.5821e+00,  1.9761e+00,  2.3702e+00,  2.7642e+00,
         3.1582e+00,  3.5522e+00,  3.9462e+00,  4.3402e+00,  4.7342e+00,
         5.1282e+00,  5.5222e+00,  5.9162e+00,  6.3102e+00,  6.7043e+00,
         7.0983e+00,  7.4923e+00,  7.8863e+00,  8.2803e+00,  8.6743e+00,
         9.0683e+00,  9.4623e+00,  9.8563e+00,  1.0250e+01,  1.0644e+01,
         1.1038e+01,  1.1432e+01,  1.1826e+01,  1.2220e+01,  1.2614e+01,
         1.3008e+01,  1.3402e+01,  1.3796e+01,  1.4190e+01,  1.4584e+01,
         1.4978e+01,  1.5372e+01,  1.5766e+01,  1.6160e+01,  1.6554e+01,
         1.6948e+01,  1.7342e+01,  1.7736e+01,  1.8131e+01,  1.8525e+01,
         1.8919e+01,  1.9313e+01,  1.9707e+01,  2.0101e+01,  2.0495e+01,
         2.0889e+01,  2.1283e+01,  2.1677e+01,  2.2071e+01,  2.2465e+01])
</pre></div>
</div>
</div>
</div>
<p>TODO</p>
<ol class="simple">
<li><p>Pyro for linear regression example</p></li>
<li><p>Handle more samples in ELBO</p></li>
<li><p>Reuse some methods</p></li>
<li><p>Add figure on reparameterization</p></li>
<li><p>Linear regression learn covariance also</p></li>
<li><p>Linear regression posterior compare with analytical posterior (refer Murphy book)</p></li>
<li><p>Clean up code and reuse code whwrever possible</p></li>
<li><p>Improve figures and make them consistent</p></li>
<li><p>Add background maths wherever needed</p></li>
<li><p>plot the Directed graphical model (refer Maths ML book and render in Pyro)</p></li>
<li><p>Look at the TFP post on <a class="reference external" href="https://www.tensorflow.org/probability/examples/Probabilistic_Layers_Regression">https://www.tensorflow.org/probability/examples/Probabilistic_Layers_Regression</a></p></li>
<li><p>Show the effect of data size (less data, solution towards prior, else dominated by likelihood)</p></li>
<li><p>Mean Firld (full covariance v/s diagonal) for surrogate</p></li>
</ol>
<p>References</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=HUsznqt2V5I">https://www.youtube.com/watch?v=HUsznqt2V5I</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=x9StQ8RZ0ag&amp;list=PLISXH-iEM4JlFsAp7trKCWyxeO3M70QyJ&amp;index=9">https://www.youtube.com/watch?v=x9StQ8RZ0ag&amp;list=PLISXH-iEM4JlFsAp7trKCWyxeO3M70QyJ&amp;index=9</a></p></li>
<li><p><a class="reference external" href="https://colab.research.google.com/github/goodboychan/goodboychan.github.io/blob/main/_notebooks/2021-09-13-02-Minimizing-KL-Divergence.ipynb#scrollTo=gd_ev8ceII8q">https://colab.research.google.com/github/goodboychan/goodboychan.github.io/blob/main/_notebooks/2021-09-13-02-Minimizing-KL-Divergence.ipynb#scrollTo=gd_ev8ceII8q</a></p></li>
<li><p><a class="reference external" href="https://goodboychan.github.io/python/coursera/tensorflow_probability/icl/2021/09/13/02-Minimizing-KL-Divergence.html">https://goodboychan.github.io/python/coursera/tensorflow_probability/icl/2021/09/13/02-Minimizing-KL-Divergence.html</a></p></li>
</ul>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "prog-ml/prog-ml.github.io",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks/variational_models"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="../information_theory/jensen-inequality.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">&lt;no title&gt;</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../bayesian_ml_with_pymc/2021-03-11-blr-pymc.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Bayesian linear regression</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By The Jupyter Book community<br/>
    
        &copy; Copyright 2022.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>